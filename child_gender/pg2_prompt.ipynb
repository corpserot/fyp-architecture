{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e71d883d",
   "metadata": {},
   "source": [
    "# PaliGemma2 Bounding Box Detection Annotation Batch Update\n",
    "Set up PaliGemma2 for prompting. It's used to perform batch updates to a\n",
    "YOLOv12 dataset using bounding box detection. To achieve this, we prompt for\n",
    "detections and parse them into YOLO format.\n",
    "\n",
    "## Input Format\n",
    "PaliGemma2-3b-mix-448 expects 448x448 input images and text prompts.\n",
    "- For bounding box tasks, the prompt describes the objects to detect.\n",
    "- The special token <image> MUST be included in the prompt\n",
    "\n",
    "## Output Format\n",
    "PaliGemma2's outputs are text-only. The way it is able to precisely express\n",
    "bounding boxes is through using special tokens <locXXXX> with XXXX being value\n",
    "within 0 and 1023. Bounding boxes will have y_min, x_min, y_max, x_max. For\n",
    "instance, an example prompt \"detect all cat ; dog\" may yield:\n",
    "\n",
    "```\n",
    "<loc0123><loc0456><loc0789><loc1023> cat;\n",
    "<loc0023><loc0656><loc0389><loc0923> dog;\n",
    "<loc0050><loc0050><loc0500><loc0500> dod;\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdcd2133",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os # convenience\n",
    "\n",
    "!nvidia-smi\n",
    "\n",
    "# Setup paths\n",
    "%cd /workspace\n",
    "home = os.getcwd()\n",
    "dataset_path = \"/workspace/dataset\"\n",
    "\n",
    "%pip install -q gdown\n",
    "%pip install -q transformers accelerate bitsandbytes pillow tqdm PyYAML hf_transfer\n",
    "%pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda8cd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gdown --fuzzy 'https://drive.google.com/file/d/1CBhRn6I4bUbxy5Z3OHmupL4qIOF24Pir/view?usp=sharing' -O {home}/dataset.zip\n",
    "!mkdir -p {home}/dataset\n",
    "!rm -rf {home}/dataset\n",
    "!unzip -q {home}/dataset.zip -d {home}/dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473ce16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_API_KEY = \"12345\" # Replace with your actual Hugging Face API key\n",
    "os.environ[\"HF_TOKEN\"] = HF_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c117c503",
   "metadata": {},
   "source": [
    "## 1. Setup PaliGemma2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aed31c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from transformers import AutoProcessor, AutoModelForImageTextToText\n",
    "from PIL import Image\n",
    "import torch\n",
    "import pg2_lib # read pg2_lib.py for more information\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model_id = \"google/paligemma2-3b-mix-448\"\n",
    "processor = AutoProcessor.from_pretrained(model_id, use_fast=True)\n",
    "model = AutoModelForImageTextToText.from_pretrained(\n",
    "  model_id, dtype=torch.bfloat16\n",
    "  ).to(device) # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6957073d",
   "metadata": {},
   "source": [
    "## 2. YOLO to Readable Bounding Box Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f6f137",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels = pg2_lib.get_class_names_from_yaml(dataset_path)\n",
    "\n",
    "# Load YOLO model once\n",
    "yolo_model_path = os.path.join(home, 'my-yolov12s.pt')\n",
    "yolo_model = pg2_lib.load_yolo_model(yolo_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1adb97f7",
   "metadata": {},
   "source": [
    "## Task A: Prompting PaliGemma2 for Bounding Box Detection\n",
    "This section iterates through the dataset, prompts PaliGemma2 for bounding box detections, and updates the YOLO annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561dddd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image_for_detection(\n",
    "  image_info: pg2_lib.YoloImageInfo,\n",
    "  yolo_model: pg2_lib.YOLO,\n",
    "  class_labels: list[str],\n",
    "  skip_bad_annotation: bool = False,\n",
    "  print_output: bool = False,\n",
    "  save_to_file: bool = True,\n",
    ") -> list[list[float]]:\n",
    "  \"\"\"\n",
    "  Processes a single image for bounding box detection using PaliGemma2.\n",
    "\n",
    "  Args:\n",
    "    image_info (YoloImageInfo): Dataclass containing image path, width, height, and YOLO annotations.\n",
    "    yolo_model (pg2_lib.YOLO): The loaded YOLO model for counting detections.\n",
    "    class_labels (list[str]): A list of class names.\n",
    "    skip_bad_annotation (bool): If True, the function returns an empty list immediately\n",
    "                                if any annotation fails to parse. Defaults to False.\n",
    "    print_output (bool): If True, print the generated text. Defaults to False.\n",
    "    save_to_file (bool): If True, save annotations to label file. Defaults to True.\n",
    "\n",
    "  Returns:\n",
    "    list[list[float]]: A list of YOLO bounding box format annotations.\n",
    "  \"\"\"\n",
    "  image_path = image_info.image_path\n",
    "  image_name = os.path.basename(image_path)\n",
    "  image_w, image_h = image_info.image_width, image_info.image_height\n",
    "  image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "  # Count detections using the YOLO model\n",
    "  class_counts = pg2_lib.count_yolo_detections(yolo_model, image_path, class_labels)\n",
    "  detection_prompt = pg2_lib.create_detection_prompt(class_labels, class_counts)\n",
    "\n",
    "  # Add <image> token to the prompt\n",
    "  inputs_for_annotations = processor(text=\"<image>\" + detection_prompt, images=image, return_tensors=\"pt\").to(device)\n",
    "\n",
    "  with torch.no_grad():\n",
    "    output_annotations = model.generate(**inputs_for_annotations, max_new_tokens=500)\n",
    "\n",
    "  generated_annotations_text = processor.decode(output_annotations[0], skip_special_tokens=True)\n",
    "  if print_output:\n",
    "    print(f\"PaliGemma2 Output for {image_name}:\\n{generated_annotations_text}\\n\")\n",
    "  try:\n",
    "    updated_bboxes = pg2_lib.parse_response_to_yolo_bbox(\n",
    "      generated_annotations_text,\n",
    "      image_w,\n",
    "      image_h,\n",
    "      class_labels,\n",
    "      model_input_size=448,\n",
    "      fail_on_error=skip_bad_annotation\n",
    "    )\n",
    "  except Exception as e:\n",
    "    print(f\"Error parsing PaliGemma2 output: {e}. Skipping annotation update for this image.\")\n",
    "    updated_bboxes = []\n",
    "\n",
    "  if save_to_file:\n",
    "    pg2_lib.save_annotations_to_file(image_name, dataset_path, updated_bboxes)\n",
    "\n",
    "  return updated_bboxes\n",
    "\n",
    "# The save_annotations_to_file function is in pg2_lib.py\n",
    "\n",
    "def display_detection(\n",
    "  image_info: pg2_lib.YoloImageInfo,\n",
    "  bboxes: list[list[float]],\n",
    "  class_labels: list[str],\n",
    "  display_width: int,\n",
    ") -> None:\n",
    "  \"\"\"\n",
    "  Displays an image with drawn bounding boxes.\n",
    "\n",
    "  Args:\n",
    "    image_info (YoloImageInfo): Dataclass containing image path, width, height.\n",
    "    bboxes (list of lists): A list of YOLO bounding box format annotations.\n",
    "    class_labels (list[str]): A list of class names.\n",
    "    display_width (int): Width to display the image.\n",
    "  \"\"\"\n",
    "  image = Image.open(image_info.image_path).convert(\"RGB\")\n",
    "  image = pg2_lib.draw_bboxes_on_image(image, bboxes, class_labels)\n",
    "  if display_width:\n",
    "    # Calculate height to maintain aspect ratio\n",
    "    aspect_ratio = image.height / image.width\n",
    "    display_height = int(display_width * aspect_ratio)\n",
    "    image = image.resize((display_width, display_height), Image.Resampling.LANCZOS)\n",
    "  display(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667a14ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_info = pg2_lib.read_yolo_dataset(dataset_path)\n",
    "assert dataset_info, \"No dataset information?!\"\n",
    "\n",
    "try_images = (0,10)\n",
    "num_try_images = try_images[1] - try_images[0]\n",
    "sample_images_with_bboxes: list[tuple[pg2_lib.YoloImageInfo, list[list[float]]]] = []\n",
    "\n",
    "for i, image_info in tqdm(enumerate(dataset_info[try_images[0]:try_images[1]])):\n",
    "  bboxes = process_image_for_detection(\n",
    "    image_info,\n",
    "    yolo_model,\n",
    "    class_labels,\n",
    "    skip_bad_annotation=False,\n",
    "    print_output=True,\n",
    "    save_to_file=False\n",
    "  )\n",
    "  sample_images_with_bboxes.append((image_info, bboxes))\n",
    "\n",
    "for image_info, bboxes in sample_images_with_bboxes:\n",
    "  if bboxes:\n",
    "    display_detection(image_info, bboxes, class_labels, display_width=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31aebfda",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_info = pg2_lib.read_yolo_dataset(dataset_path)\n",
    "assert dataset_info, \"No dataset information?!\"\n",
    "\n",
    "for i, image_info in tqdm(enumerate(dataset_info)):\n",
    "  bboxes = process_image_for_detection(\n",
    "    image_info,\n",
    "    yolo_model,\n",
    "    class_labels,\n",
    "    skip_bad_annotation=False,\n",
    "    print_output=False\n",
    "  )\n",
    "  # Saving is done inside the function"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
