{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35c3e442",
   "metadata": {},
   "source": [
    "# PaliGemma2 Segmentation Annotation Batch Update\n",
    "Set up PaliGemma2 for prompting. It's used to perform batch updates to a\n",
    "YOLOv12 dataset. To achieve this, we need to convert the YOLO format into a\n",
    "readable format for PaliGemma2 and reverse it when storing them.\n",
    "\n",
    "## Input Format Documentation\n",
    "PaliGemma2-3b-mix-448 expects 448x448 input images and text prompts.\n",
    "- For bounding box tasks, the prompt often describes the objects to detect or\n",
    "asks for bounding box coordinates in a human-readable format.\n",
    "- For segmentation tasks, the prompt typically asks the model to produce a\n",
    "pixel level mask or describe the regions belonging to specific object\n",
    "classes in a clear and structured format.\n",
    "\n",
    "YOLOv12 bounding box training uses annotations in the format `[class_id,\n",
    "x_center, y_center, width, height]`, with all values normalized to the image\n",
    "dimensions. Training images are commonly resized to a fixed square resolution\n",
    "such as 640x640.\n",
    "\n",
    "For segmentation tasks, YOLOv12 typically relies on polygon based annotations\n",
    "or binary masks that correspond to each object instance, and these masks are\n",
    "trained alongside the images in the same normalized coordinate space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42273a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os # convenience\n",
    "\n",
    "!nvidia-smi\n",
    "\n",
    "# Setup paths\n",
    "%cd /workspace\n",
    "home = os.getcwd()\n",
    "dataset_path = \"/workspace/dataset\"\n",
    "\n",
    "%pip install -q gdown\n",
    "%pip install -q transformers accelerate bitsandbytes pillow tqdm PyYAML hf_transfer\n",
    "%pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ae11b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gdown --fuzzy 'https://drive.google.com/file/d/1tvFnEur7PQjgsidcUIeYpzMSoAs--GsS/view?usp=sharing' -O {home}/dataset.zip\n",
    "!mkdir -p {home}/dataset\n",
    "!rm -rf {home}/dataset\n",
    "!unzip -q {home}/dataset.zip -d {home}/dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25602a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_API_KEY = \"12345\" # Replace with your actual Hugging Face API key\n",
    "os.environ[\"HF_TOKEN\"] = HF_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70eca279",
   "metadata": {},
   "source": [
    "## 1. Setup PaliGemma2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0c6588",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from transformers import AutoProcessor, AutoModelForImageTextToText\n",
    "from PIL import Image\n",
    "import torch\n",
    "import pg2_lib # read pg2_lib.py for more information\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model_id = \"google/paligemma2-3b-mix-448\"\n",
    "processor = AutoProcessor.from_pretrained(model_id, use_fast=True)\n",
    "model = AutoModelForImageTextToText.from_pretrained(\n",
    "  model_id, dtype=torch.bfloat16\n",
    "  ).to(device) # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5ceb90",
   "metadata": {},
   "source": [
    "## 2. YOLO to Readable Bounding Box Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2cd9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels = pg2_lib.get_class_names_from_yaml(dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16cb7eee",
   "metadata": {},
   "source": [
    "## Task A: Prompting PaliGemma2 for Segmentation Polygons\n",
    "This section iterates through the dataset, prompts PaliGemma2 for polygon segmentation annotations, and updates the YOLO annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240fdbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image_for_segmentation(\n",
    "  image_info: pg2_lib.YoloImageInfo,\n",
    "  skip_bad_annotation: bool = False,\n",
    "  print_output: bool = False,\n",
    ") -> list[list[float]]:\n",
    "  \"\"\"\n",
    "  Processes a single image for segmentation using PaliGemma2 and saves the annotations.\n",
    "\n",
    "  Args:\n",
    "    image_info (YoloImageInfo): Dataclass containing image path, width, height, and YOLO annotations.\n",
    "    skip_bad_annotation (bool): If True, the function returns an empty list immediately\n",
    "                                if any annotation fails to parse. Defaults to False.\n",
    "\n",
    "  Returns:\n",
    "    list[list[float]]: A list of YOLO segmentation format annotations.\n",
    "  \"\"\"\n",
    "  image_path = image_info.image_path\n",
    "  image_name = os.path.basename(image_path)\n",
    "  image_w, image_h = image_info.image_width, image_info.image_height\n",
    "  image = Image.open(image_path).convert(\"RGB\")\n",
    "  yolo_annotations = image_info.yolo_annotations\n",
    "\n",
    "  readable_bboxes = pg2_lib.yolo_to_readable_bbox(\n",
    "  yolo_annotations,\n",
    "  image_w,\n",
    "  image_h,\n",
    "  class_labels,\n",
    "  model_input_size=448 # Pass the model's expected input size\n",
    "  )\n",
    "  # No longer using readable_bboxes in the prompt\n",
    "  segmentation_prompt = pg2_lib.create_segmentation_prompt(class_labels)\n",
    "\n",
    "  # Add <image> token to the prompt\n",
    "  inputs_for_annotations = processor(text=\"<image>\" + segmentation_prompt, images=image, return_tensors=\"pt\").to(device)\n",
    "\n",
    "  with torch.no_grad():\n",
    "    output_annotations = model.generate(**inputs_for_annotations, max_new_tokens=500)\n",
    "\n",
    "  generated_annotations_text = processor.decode(output_annotations[0], skip_special_tokens=True)\n",
    "  if print_output:\n",
    "    print(f\"PaliGemma2 Output for {image_name}:\\n{generated_annotations_text}\\n\")\n",
    "  try:\n",
    "    updated_segmentations = pg2_lib.parse_response_to_yolo_segmentation(\n",
    "      generated_annotations_text,\n",
    "      image_w,\n",
    "      image_h,\n",
    "      class_labels,\n",
    "      model_input_size=448, # Pass the model's expected input size\n",
    "      fail_on_error=skip_bad_annotation\n",
    "    )\n",
    "  except Exception as e:\n",
    "    print(f\"Error parsing PaliGemma2 output: {e}. Skipping annotation update for this image.\")\n",
    "    updated_segmentations = []\n",
    "\n",
    "  output_label_path = os.path.join(dataset_path, \"labels\", os.path.splitext(image_name)[0] + \".txt\")\n",
    "  if updated_segmentations:\n",
    "    with open(output_label_path, 'w') as f:\n",
    "      for ann in updated_segmentations:\n",
    "        f.write(f\"{int(ann[0])} {' '.join(map(str, ann[1:]))}\\n\")\n",
    "  return updated_segmentations\n",
    "\n",
    "# The save_segmentations_to_file function is moved to pg2_lib.py\n",
    "# def save_segmentations_to_file(...) -> None:\n",
    "#   ...\n",
    "\n",
    "def display_segmentation(\n",
    "  image_info: pg2_lib.YoloImageInfo,\n",
    "  segmentations: list[list[float]],\n",
    "  display_width: int,\n",
    ") -> None:\n",
    "  \"\"\"\n",
    "  Displays an image with drawn segmentation polygons.\n",
    "\n",
    "  Args:\n",
    "    image_info (YoloImageInfo): Dataclass containing image path, width, height.\n",
    "    segmentations (list of lists): A list of YOLO segmentation format annotations.\n",
    "    display_width (int): Width to display the image.\n",
    "  \"\"\"\n",
    "  image = Image.open(image_info.image_path).convert(\"RGB\")\n",
    "  image = pg2_lib.draw_segmentations_on_image(image, segmentations, class_labels)\n",
    "  if display_width:\n",
    "    # Calculate height to maintain aspect ratio\n",
    "    aspect_ratio = image.height / image.width\n",
    "    display_height = int(display_width * aspect_ratio)\n",
    "    image = image.resize((display_width, display_height), Image.Resampling.LANCZOS)\n",
    "  display(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f6d9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_info = pg2_lib.read_yolo_dataset(dataset_path)\n",
    "assert dataset_info, \"No dataset information?!\"\n",
    "\n",
    "try_images = (0,10)\n",
    "num_try_images = try_images[1] - try_images[0]\n",
    "sample_images_with_segmentations: list[tuple[pg2_lib.YoloImageInfo, list[list[float]]]] = []\n",
    "\n",
    "for i, image_info in tqdm(enumerate(dataset_info[try_images[0]:try_images[1]])):\n",
    "  segmentations = process_image_for_segmentation(image_info, skip_bad_annotation=False, print_output=True)\n",
    "  sample_images_with_segmentations.append((image_info, segmentations))\n",
    "\n",
    "for image_info, segmentations in sample_images_with_segmentations:\n",
    "  if segmentations:\n",
    "    display_segmentation(image_info, segmentations, display_width=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0171954",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_info = pg2_lib.read_yolo_dataset(dataset_path)\n",
    "assert dataset_info, \"No dataset information?!\"\n",
    "\n",
    "for i, image_info in tqdm(enumerate(dataset_info)):\n",
    "  segmentations = process_image_for_segmentation(image_info, skip_bad_annotation=False, print_output=False)\n",
    "  image_name = os.path.basename(image_info.image_path)\n",
    "  pg2_lib.save_segmentations_to_file(image_name, dataset_path, segmentations)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
