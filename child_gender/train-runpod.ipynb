{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#**How to Train YOLOv12 Object Detection Model on a Custom Dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 01 # Install the Ultralytics Package**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Nov 11 16:58:07 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.127.08             Driver Version: 550.127.08     CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA RTX A5000               On  |   00000000:D2:00.0 Off |                  Off |\n",
      "| 30%   22C    P8             19W /  230W |       2MiB /  24564MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "/workspace\n",
      "Requirement already satisfied: ultralytics in /usr/local/lib/python3.12/dist-packages (8.3.227)\n",
      "Requirement already satisfied: gdown in /usr/local/lib/python3.12/dist-packages (5.2.0)\n",
      "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.1.2)\n",
      "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (3.10.7)\n",
      "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (4.12.0.88)\n",
      "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (11.0.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (6.0.3)\n",
      "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.32.5)\n",
      "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.16.3)\n",
      "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.8.0+cu128)\n",
      "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (0.23.0+cu128)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from ultralytics) (7.1.0)\n",
      "Requirement already satisfied: polars in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.35.2)\n",
      "Requirement already satisfied: ultralytics-thop>=2.0.18 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.0.18)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from gdown) (4.14.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from gdown) (3.20.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from gdown) (4.67.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (25.0)\n",
      "Requirement already satisfied: pyparsing>=3 in /usr/lib/python3/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2025.10.5)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (2024.6.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.4.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.8.0->ultralytics) (1.3.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->gdown) (2.8)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.3)\n",
      "Requirement already satisfied: polars-runtime-32==1.35.2 in /usr/local/lib/python3.12/dist-packages (from polars->ultralytics) (1.35.2)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import locale\n",
    "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
    "\n",
    "!nvidia-smi\n",
    "\n",
    "# Change working directory for RunPod environment\n",
    "%cd /workspace\n",
    "home = os.getcwd()\n",
    "\n",
    "%pip install ultralytics gdown\n",
    "# !curl https://rclone.org/install.sh | sudo bash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 02 # Import All the Requried Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.227 ðŸš€ Python-3.12.3 torch-2.8.0+cu128 CUDA:0 (NVIDIA RTX A5000, 24241MiB)\n",
      "Setup complete âœ… (96 CPUs, 503.5 GB RAM, 0.6/30.0 GB disk)\n"
     ]
    }
   ],
   "source": [
    "import ultralytics\n",
    "ultralytics.checks()\n",
    "\n",
    "from ultralytics import YOLO\n",
    "from IPython.display import Image\n",
    "\n",
    "# Adjust dataset path for RunPod environment\n",
    "dataset_path = f'/{home}/dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1p18vaRuXzmtqNrV2udIWE8wDimLYbfRp\n",
      "From (redirected): https://drive.google.com/uc?id=1p18vaRuXzmtqNrV2udIWE8wDimLYbfRp&confirm=t&uuid=f30d006d-5213-4b04-97a6-e108aee4b6bf\n",
      "To: /workspace/dataset.zip\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.84G/1.84G [00:39<00:00, 46.7MB/s]\n",
      "//workspace/dataset\n",
      "total 6845\n",
      "drwxrwxrwx 4 root root 2003987 Nov 10 16:57 test\n",
      "drwxrwxrwx 4 root root 3000166 Nov 10 16:57 train\n",
      "drwxrwxrwx 4 root root 2004146 Nov 10 16:57 valid\n"
     ]
    }
   ],
   "source": [
    "# Assuming dataset.zip is in /workspace\n",
    "!gdown --fuzzy 'https://drive.google.com/file/d/1p18vaRuXzmtqNrV2udIWE8wDimLYbfRp/view?usp=sharing' -O {home}/dataset.zip\n",
    "!unzip -q {home}/dataset.zip -d {home}/temp-dataset\n",
    "!mv {home}/temp-dataset/*/ {dataset_path}\n",
    "\n",
    "!echo {dataset_path}\n",
    "!ls -lA {dataset_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step # 03 Fine-tune YOLOv12 model on a Custom Dataset**\n",
    "We are now ready to fine-tune our YOLOv12 model. In the code below, we initialize the model using a starting checkpointâ€”here, we use `yolov12s.yaml`, but you can replace it with any other model (e.g., `yolov12n.pt`, `yolov12m.pt`, `yolov12l.pt`, or `yolov12x.pt`) based on your preference. We set the training to run for 50 epochs in this example; however, you should adjust the number of epochs along with other hyperparameters such as batch size, image size, and augmentation settings (scale, mosaic, mixup, and copy-paste) based on your hardware capabilities and dataset size.\n",
    "\n",
    "\n",
    "\n",
    "**Note:** **Note that after training, you might encounter a `TypeError: argument of type 'PosixPath' is not iterable error` â€” this is a known issue, but your model weights will still be saved, so you can safely proceed to running inference.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.227 ðŸš€ Python-3.12.3 torch-2.8.0+cu128 CUDA:0 (NVIDIA RTX A5000, 24241MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=64, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=//workspace/dataset/data.yaml, degrees=0.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=50, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolo12x.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=train2, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/workspace/runs/detect/train2, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=4\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1      2784  ultralytics.nn.modules.conv.Conv             [3, 96, 3, 2]                 \n",
      "  1                  -1  1    166272  ultralytics.nn.modules.conv.Conv             [96, 192, 3, 2]               \n",
      "  2                  -1  2    389760  ultralytics.nn.modules.block.C3k2            [192, 384, 2, True, 0.25]     \n",
      "  3                  -1  1   1327872  ultralytics.nn.modules.conv.Conv             [384, 384, 3, 2]              \n",
      "  4                  -1  2   1553664  ultralytics.nn.modules.block.C3k2            [384, 768, 2, True, 0.25]     \n",
      "  5                  -1  1   5309952  ultralytics.nn.modules.conv.Conv             [768, 768, 3, 2]              \n",
      "  6                  -1  4   9512128  ultralytics.nn.modules.block.A2C2f           [768, 768, 4, True, 4, True, 1.2]\n",
      "  7                  -1  1   5309952  ultralytics.nn.modules.conv.Conv             [768, 768, 3, 2]              \n",
      "  8                  -1  4   9512128  ultralytics.nn.modules.block.A2C2f           [768, 768, 4, True, 1, True, 1.2]\n",
      "  9                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 10             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 11                  -1  2   4727040  ultralytics.nn.modules.block.A2C2f           [1536, 768, 2, False, -1, True, 1.2]\n",
      " 12                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 13             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 14                  -1  2   1331328  ultralytics.nn.modules.block.A2C2f           [1536, 384, 2, False, -1, True, 1.2]\n",
      " 15                  -1  1   1327872  ultralytics.nn.modules.conv.Conv             [384, 384, 3, 2]              \n",
      " 16            [-1, 11]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 17                  -1  2   4579584  ultralytics.nn.modules.block.A2C2f           [1152, 768, 2, False, -1, True, 1.2]\n",
      " 18                  -1  1   5309952  ultralytics.nn.modules.conv.Conv             [768, 768, 3, 2]              \n",
      " 19             [-1, 8]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 20                  -1  2   5612544  ultralytics.nn.modules.block.C3k2            [1536, 768, 2, True]          \n",
      " 21        [14, 17, 20]  1   3150172  ultralytics.nn.modules.head.Detect           [4, [384, 768, 768]]          \n",
      "YOLOv12x summary: 488 layers, 59,123,004 parameters, 59,122,988 gradients, 199.8 GFLOPs\n",
      "\n",
      "Transferred 1239/1245 items from pretrained weights\n",
      "Freezing layer 'model.21.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access âœ… (ping: 0.4Â±0.2 ms, read: 26.5Â±17.1 MB/s, size: 92.2 KB)\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /workspace/dataset/train/labels... 19278 images, 0 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 19278/19278 536.0it/s 36.0s\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /workspace/dataset/train/labels.cache\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.4Â±0.3 ms, read: 28.4Â±15.5 MB/s, size: 116.4 KB)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /workspace/dataset/valid/labels... 402 images, 0 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 402/402 511.6it/s 0.8s\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /workspace/dataset/valid/labels.cache\n",
      "Plotting labels to /workspace/runs/detect/train2/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01, momentum=0.9) with parameter groups 205 weight(decay=0.0), 214 weight(decay=0.0005), 211 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1m/workspace/runs/detect/train2\u001b[0m\n",
      "Starting training for 50 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      ": 0% â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 0/302  0.5s\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 76.00 MiB. GPU 0 has a total capacity of 23.67 GiB of which 26.25 MiB is free. Process 635165 has 23.61 GiB memory in use. Of the allocated memory 23.17 GiB is allocated by PyTorch, and 127.91 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# choose one:\u001b[39;00m\n\u001b[32m      2\u001b[39m \n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# model = YOLO(\"yolo12n.pt\")\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# model = YOLO(\"yolo12s.pt\")\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# model = YOLO(\"yolo12m.pt\")\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# model = YOLO(\"yolo12l.pt\")\u001b[39;00m\n\u001b[32m      7\u001b[39m model = YOLO(\u001b[33m\"\u001b[39m\u001b[33myolo12x.pt\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m results = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdataset_path\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/data.yaml\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m64\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# cos_lr=True,\u001b[39;49;00m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# lr0=0.005,\u001b[39;49;00m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# optimizer=\"AdamW\",\u001b[39;49;00m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# weight_decay=0.01,\u001b[39;49;00m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# momentum=0.09,\u001b[39;49;00m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# patience=80,\u001b[39;49;00m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# deterministic=False,\u001b[39;49;00m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# warmup_epochs=3,\u001b[39;49;00m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# warmup_bias_lr=0.1,\u001b[39;49;00m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# iou=0.75,\u001b[39;49;00m\n\u001b[32m     23\u001b[39m \n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# auto_augment=\"randaugment\",\u001b[39;49;00m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# mosaic=0.4,\u001b[39;49;00m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# mixup=0.1,\u001b[39;49;00m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# close_mosaic=10,\u001b[39;49;00m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# copy_paste=0.1,\u001b[39;49;00m\n\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# erasing=0.4,\u001b[39;49;00m\n\u001b[32m     30\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# hsv_h=0.015, hsv_s=0.5, hsv_v=0.4,\u001b[39;49;00m\n\u001b[32m     31\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# scale=0.2,\u001b[39;49;00m\n\u001b[32m     32\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# translate=0.1,\u001b[39;49;00m\n\u001b[32m     33\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# degrees=15,\u001b[39;49;00m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# flipud=0, # disable\u001b[39;49;00m\n\u001b[32m     35\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/ultralytics/engine/model.py:778\u001b[39m, in \u001b[36mModel.train\u001b[39m\u001b[34m(self, trainer, **kwargs)\u001b[39m\n\u001b[32m    775\u001b[39m     \u001b[38;5;28mself\u001b[39m.trainer.model = \u001b[38;5;28mself\u001b[39m.trainer.get_model(weights=\u001b[38;5;28mself\u001b[39m.model \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ckpt \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, cfg=\u001b[38;5;28mself\u001b[39m.model.yaml)\n\u001b[32m    776\u001b[39m     \u001b[38;5;28mself\u001b[39m.model = \u001b[38;5;28mself\u001b[39m.trainer.model\n\u001b[32m--> \u001b[39m\u001b[32m778\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    779\u001b[39m \u001b[38;5;66;03m# Update model and cfg after training\u001b[39;00m\n\u001b[32m    780\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m RANK \u001b[38;5;129;01min\u001b[39;00m {-\u001b[32m1\u001b[39m, \u001b[32m0\u001b[39m}:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/ultralytics/engine/trainer.py:243\u001b[39m, in \u001b[36mBaseTrainer.train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    240\u001b[39m         ddp_cleanup(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mstr\u001b[39m(file))\n\u001b[32m    242\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m243\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_do_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/ultralytics/engine/trainer.py:427\u001b[39m, in \u001b[36mBaseTrainer._do_train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    425\u001b[39m     loss, \u001b[38;5;28mself\u001b[39m.loss_items = unwrap_model(\u001b[38;5;28mself\u001b[39m.model).loss(batch, preds)\n\u001b[32m    426\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m427\u001b[39m     loss, \u001b[38;5;28mself\u001b[39m.loss_items = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    428\u001b[39m \u001b[38;5;28mself\u001b[39m.loss = loss.sum()\n\u001b[32m    429\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m RANK != -\u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/ultralytics/nn/tasks.py:136\u001b[39m, in \u001b[36mBaseModel.forward\u001b[39m\u001b[34m(self, x, *args, **kwargs)\u001b[39m\n\u001b[32m    123\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Perform forward pass of the model for either training or inference.\u001b[39;00m\n\u001b[32m    124\u001b[39m \n\u001b[32m    125\u001b[39m \u001b[33;03mIf x is a dict, calculates and returns the loss for training. Otherwise, returns predictions for inference.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    133\u001b[39m \u001b[33;03m    (torch.Tensor): Loss if x is a dict (training), or network predictions (inference).\u001b[39;00m\n\u001b[32m    134\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):  \u001b[38;5;66;03m# for cases of training and validating while training.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    137\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.predict(x, *args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/ultralytics/nn/tasks.py:327\u001b[39m, in \u001b[36mBaseModel.loss\u001b[39m\u001b[34m(self, batch, preds)\u001b[39m\n\u001b[32m    324\u001b[39m     \u001b[38;5;28mself\u001b[39m.criterion = \u001b[38;5;28mself\u001b[39m.init_criterion()\n\u001b[32m    326\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m preds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m     preds = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mimg\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    328\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.criterion(preds, batch)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/ultralytics/nn/tasks.py:137\u001b[39m, in \u001b[36mBaseModel.forward\u001b[39m\u001b[34m(self, x, *args, **kwargs)\u001b[39m\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):  \u001b[38;5;66;03m# for cases of training and validating while training.\u001b[39;00m\n\u001b[32m    136\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.loss(x, *args, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/ultralytics/nn/tasks.py:154\u001b[39m, in \u001b[36mBaseModel.predict\u001b[39m\u001b[34m(self, x, profile, visualize, augment, embed)\u001b[39m\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m augment:\n\u001b[32m    153\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._predict_augment(x)\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_predict_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprofile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/ultralytics/nn/tasks.py:176\u001b[39m, in \u001b[36mBaseModel._predict_once\u001b[39m\u001b[34m(self, x, profile, visualize, embed)\u001b[39m\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m profile:\n\u001b[32m    175\u001b[39m     \u001b[38;5;28mself\u001b[39m._profile_one_layer(m, x, dt)\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m x = \u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# run\u001b[39;00m\n\u001b[32m    177\u001b[39m y.append(x \u001b[38;5;28;01mif\u001b[39;00m m.i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.save \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# save output\u001b[39;00m\n\u001b[32m    178\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m visualize:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/ultralytics/nn/modules/block.py:306\u001b[39m, in \u001b[36mC2f.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    304\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Forward pass through C2f layer.\"\"\"\u001b[39;00m\n\u001b[32m    305\u001b[39m y = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m.cv1(x).chunk(\u001b[32m2\u001b[39m, \u001b[32m1\u001b[39m))\n\u001b[32m--> \u001b[39m\u001b[32m306\u001b[39m \u001b[43my\u001b[49m\u001b[43m.\u001b[49m\u001b[43mextend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mm\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    307\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cv2(torch.cat(y, \u001b[32m1\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/ultralytics/nn/modules/block.py:306\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    304\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Forward pass through C2f layer.\"\"\"\u001b[39;00m\n\u001b[32m    305\u001b[39m y = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m.cv1(x).chunk(\u001b[32m2\u001b[39m, \u001b[32m1\u001b[39m))\n\u001b[32m--> \u001b[39m\u001b[32m306\u001b[39m y.extend(\u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.m)\n\u001b[32m    307\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cv2(torch.cat(y, \u001b[32m1\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/ultralytics/nn/modules/block.py:340\u001b[39m, in \u001b[36mC3.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    338\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch.Tensor) -> torch.Tensor:\n\u001b[32m    339\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Forward pass through the CSP bottleneck with 3 convolutions.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m340\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cv3(torch.cat((\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m.cv2(x)), \u001b[32m1\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py:244\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    242\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    243\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m244\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    245\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/ultralytics/nn/modules/block.py:476\u001b[39m, in \u001b[36mBottleneck.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    474\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch.Tensor) -> torch.Tensor:\n\u001b[32m    475\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Apply bottleneck with optional shortcut connection.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m476\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m x + \u001b[38;5;28mself\u001b[39m.cv2(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.add \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cv2(\u001b[38;5;28mself\u001b[39m.cv1(x))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/ultralytics/nn/modules/conv.py:78\u001b[39m, in \u001b[36mConv.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m     70\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Apply convolution, batch normalization and activation to input tensor.\u001b[39;00m\n\u001b[32m     71\u001b[39m \n\u001b[32m     72\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     76\u001b[39m \u001b[33;03m        (torch.Tensor): Output tensor.\u001b[39;00m\n\u001b[32m     77\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.act(\u001b[38;5;28mself\u001b[39m.bn(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:548\u001b[39m, in \u001b[36mConv2d.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    547\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m548\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:543\u001b[39m, in \u001b[36mConv2d._conv_forward\u001b[39m\u001b[34m(self, input, weight, bias)\u001b[39m\n\u001b[32m    531\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.padding_mode != \u001b[33m\"\u001b[39m\u001b[33mzeros\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    532\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.conv2d(\n\u001b[32m    533\u001b[39m         F.pad(\n\u001b[32m    534\u001b[39m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m._reversed_padding_repeated_twice, mode=\u001b[38;5;28mself\u001b[39m.padding_mode\n\u001b[32m   (...)\u001b[39m\u001b[32m    541\u001b[39m         \u001b[38;5;28mself\u001b[39m.groups,\n\u001b[32m    542\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m543\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    544\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgroups\u001b[49m\n\u001b[32m    545\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 76.00 MiB. GPU 0 has a total capacity of 23.67 GiB of which 26.25 MiB is free. Process 635165 has 23.61 GiB memory in use. Of the allocated memory 23.17 GiB is allocated by PyTorch, and 127.91 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# choose one:\n",
    "\n",
    "# model = YOLO(\"yolo12n.pt\")\n",
    "# model = YOLO(\"yolo12s.pt\")\n",
    "# model = YOLO(\"yolo12m.pt\")\n",
    "# model = YOLO(\"yolo12l.pt\")\n",
    "model = YOLO(\"yolo12x.pt\")\n",
    "\n",
    "results = model.train(\n",
    "    data=f'{dataset_path}/data.yaml',\n",
    "    batch=64,\n",
    "    # cos_lr=True,\n",
    "    # lr0=0.005,\n",
    "    # optimizer=\"AdamW\",\n",
    "    # weight_decay=0.01,\n",
    "    # momentum=0.09,\n",
    "    epochs=50,\n",
    "    # patience=80,\n",
    "    # deterministic=False,\n",
    "    # warmup_epochs=3,\n",
    "    # warmup_bias_lr=0.1,\n",
    "    # iou=0.75,\n",
    "\n",
    "    # auto_augment=\"randaugment\",\n",
    "    # mosaic=0.4,\n",
    "    # mixup=0.1,\n",
    "    # close_mosaic=10,\n",
    "    # copy_paste=0.1,\n",
    "    # erasing=0.4,\n",
    "    # hsv_h=0.015, hsv_s=0.5, hsv_v=0.4,\n",
    "    # scale=0.2,\n",
    "    # translate=0.1,\n",
    "    # degrees=15,\n",
    "    # flipud=0, # disable\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.227 ðŸš€ Python-3.12.3 torch-2.8.0+cu128 CUDA:0 (NVIDIA RTX A5000, 24241MiB)\n",
      "YOLOv12n summary (fused): 159 layers, 2,557,508 parameters, 0 gradients, 6.3 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.8Â±0.1 ms, read: 19.3Â±12.1 MB/s, size: 100.3 KB)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /workspace/dataset/valid/labels.cache... 402 images, 0 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 402/402 25.2Mit/s 0.0s\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 26/26 6.8it/s 3.8s\n",
      "                   all        402        759      0.658       0.59       0.63      0.406\n",
      "                   boy        218        278      0.616      0.586      0.622       0.42\n",
      "                  girl        202        259      0.659      0.568      0.618      0.389\n",
      "                   man         85         90      0.737       0.53      0.627      0.416\n",
      "                 woman        112        132      0.621      0.674      0.653      0.401\n",
      "Speed: 1.0ms preprocess, 2.8ms inference, 0.0ms loss, 0.9ms postprocess per image\n",
      "Results saved to \u001b[1m/workspace/runs/detect/val\u001b[0m\n",
      "\n",
      "image 1/402 //workspace/dataset/test/images/000000000974_jpg.rf.558237f95e7aae30217907961ed4a81b_orig.jpg: 640x640 1 boy, 1 man, 9.4ms\n",
      "image 2/402 //workspace/dataset/test/images/000000001592_jpg.rf.ed575cca68e79fe043552a0b1f71edfd_orig.jpg: 640x640 2 girls, 1 woman, 8.4ms\n",
      "image 3/402 //workspace/dataset/test/images/000000248965_jpg.rf.1a3dcd6322a8ee76f6ae26bf79a8497d_orig.jpg: 640x640 2 girls, 8.3ms\n",
      "image 4/402 //workspace/dataset/test/images/00be2519a80802d9_jpg.rf.76dae1ee196bf6439174faebaa52aeed_orig.jpg: 640x640 1 girl, 8.4ms\n",
      "image 5/402 //workspace/dataset/test/images/08c999e41f81cff9_jpg.rf.7765e62f3d1c63f4f69c5bd8eef7a809_orig.jpg: 640x640 1 boy, 2 girls, 11.3ms\n",
      "image 6/402 //workspace/dataset/test/images/0b0ce543f5078da0_jpg.rf.507bffab75adb439d3d4608f07b32b21_orig.jpg: 640x640 1 boy, 11.1ms\n",
      "image 7/402 //workspace/dataset/test/images/0c435dc1cc3faecf_jpg.rf.bf2d9de293f4b3cda641552ed5e8931a_orig.jpg: 640x640 1 boy, 15.6ms\n",
      "image 8/402 //workspace/dataset/test/images/0cfaafae555fe90c_jpg.rf.31a43cc9651d644a913e9c353e5245ca_orig.jpg: 640x640 2 boys, 2 girls, 17.7ms\n",
      "image 9/402 //workspace/dataset/test/images/0d155561861fa6ac_jpg.rf.561c565730c33050e80b222b67e1cd2b_orig.jpg: 640x640 1 boy, 2 mans, 2 womans, 19.8ms\n",
      "image 10/402 //workspace/dataset/test/images/0dea708a2b6a04c0_jpg.rf.c872ad2b66b39525f44ea0778d6e76be_orig.jpg: 640x640 2 boys, 1 girl, 1 man, 3 womans, 18.8ms\n",
      "image 11/402 //workspace/dataset/test/images/10000000_7505963559480713_8606382332440427413_n_mp4-0017_jpg.rf.16e4727904f14b23b9cebf02089c12bc_orig.jpg: 640x640 1 boy, 1 woman, 19.0ms\n",
      "image 12/402 //workspace/dataset/test/images/10000000_7505963559480713_8606382332440427413_n_mp4-0018_jpg.rf.a9ec75f66f86535355e7e06eab43e655_orig.jpg: 640x640 1 boy, 1 woman, 19.4ms\n",
      "image 13/402 //workspace/dataset/test/images/10000000_7505963559480713_8606382332440427413_n_mp4-0034_jpg.rf.9c3441c159205c55c66fb278b84d0c66_orig.jpg: 640x640 1 boy, 1 woman, 18.0ms\n",
      "image 14/402 //workspace/dataset/test/images/10000000_7505963559480713_8606382332440427413_n_mp4-0035_jpg.rf.1a8b38d973f3a2568ae1853a49e48980_orig.jpg: 640x640 1 boy, 1 woman, 17.0ms\n",
      "image 15/402 //workspace/dataset/test/images/10000000_7505963559480713_8606382332440427413_n_mp4-0038_jpg.rf.6a0cd34aece0d153db189c6129456776_orig.jpg: 640x640 1 boy, 1 woman, 16.2ms\n",
      "image 16/402 //workspace/dataset/test/images/10029_jpg.rf.0dba1af1a1b248820d2fef4872738ba9_orig.jpg: 640x640 1 girl, 16.0ms\n",
      "image 17/402 //workspace/dataset/test/images/10047_jpg.rf.ba393dde8f7300d951e6213df59b6acd_orig.jpg: 640x640 2 girls, 2 womans, 16.2ms\n",
      "image 18/402 //workspace/dataset/test/images/10075_jpg.rf.9891934e03f917804b6aab11fec4e973_orig.jpg: 640x640 2 girls, 14.8ms\n",
      "image 19/402 //workspace/dataset/test/images/10075_jpg.rf.b7a4501384145f28033fe3eaa1de3940_orig.jpg: 640x640 2 girls, 16.7ms\n",
      "image 20/402 //workspace/dataset/test/images/10099_jpg.rf.b91e433fbf571af1762b173577d19197_orig.jpg: 640x640 1 boy, 14.6ms\n",
      "image 21/402 //workspace/dataset/test/images/10199_jpg.rf.992a67997531382b470d5101c54b9c3d_orig.jpg: 640x640 1 girl, 1 woman, 16.3ms\n",
      "image 22/402 //workspace/dataset/test/images/10225_jpg.rf.fd5a46c8d44325319357947eadef1319_orig.jpg: 640x640 1 girl, 16.3ms\n",
      "image 23/402 //workspace/dataset/test/images/10297_jpg.rf.641bebda8a2fde9d9c975f466bbb021c_orig.jpg: 640x640 2 girls, 1 man, 15.0ms\n",
      "image 24/402 //workspace/dataset/test/images/10300_jpg.rf.ac83c3ddf671dd3ed9d8b568365b897e_orig.jpg: 640x640 1 boy, 2 girls, 15.3ms\n",
      "image 25/402 //workspace/dataset/test/images/10555_jpg.rf.07829fdbd4716f9ece89da14f011b61b_orig.jpg: 640x640 2 boys, 1 woman, 18.7ms\n",
      "image 26/402 //workspace/dataset/test/images/10555_jpg.rf.762051d60d1c69a1798870d079d8d7ce_orig.jpg: 640x640 1 boy, 1 woman, 15.6ms\n",
      "image 27/402 //workspace/dataset/test/images/10715_jpg.rf.e406716fee577e22029882567111d16b_orig.jpg: 640x640 1 boy, 15.5ms\n",
      "image 28/402 //workspace/dataset/test/images/10772_jpg.rf.ac1884eda46b63f88e52bc674d48af87_orig.jpg: 640x640 1 boy, 12.2ms\n",
      "image 29/402 //workspace/dataset/test/images/107_jpg.rf.ffd80b77176e9e3df1e8757b43965587_orig.jpg: 640x640 1 boy, 9.3ms\n",
      "image 30/402 //workspace/dataset/test/images/108_jpg.rf.7d6d66e0dfb06ed6a68b2b83facd6e37_orig.jpg: 640x640 1 boy, 1 girl, 9.2ms\n",
      "image 31/402 //workspace/dataset/test/images/109_jpg.rf.63af5420c3564aa193f12bbdad38247e_orig.jpg: 640x640 1 boy, 9.2ms\n",
      "image 32/402 //workspace/dataset/test/images/11116_jpg.rf.1ca23f358a39150a9399205b2df0ae3f_orig.jpg: 640x640 1 boy, 9.2ms\n",
      "image 33/402 //workspace/dataset/test/images/11376_jpg.rf.542d1a762fcdcbb88314d5b02f5e825f_orig.jpg: 640x640 1 boy, 9.3ms\n",
      "image 34/402 //workspace/dataset/test/images/11437_jpg.rf.d10598bfa3ebde52f56684b7c19c38df_orig.jpg: 640x640 1 boy, 1 woman, 9.2ms\n",
      "image 35/402 //workspace/dataset/test/images/11493_jpg.rf.559fcf930a904560e2a5a2cbd52d2b61_orig.jpg: 640x640 1 boy, 9.4ms\n",
      "image 36/402 //workspace/dataset/test/images/114_PNG_jpg.rf.9a9592ad4647e90e757983b700ddef6f_orig.jpg: 640x640 1 girl, 2 womans, 9.2ms\n",
      "image 37/402 //workspace/dataset/test/images/11538_jpg.rf.27ad00db3cb6da026c157896b2d0b703_orig.jpg: 640x640 1 girl, 9.4ms\n",
      "image 38/402 //workspace/dataset/test/images/11542_jpg.rf.d445e086ce02a9f8e1561f82b991721f_orig.jpg: 640x640 1 girl, 9.7ms\n",
      "image 39/402 //workspace/dataset/test/images/11625_jpg.rf.3a8f49b92bc136afcbd7fa5e31aff0e3_orig.jpg: 640x640 2 boys, 1 girl, 1 man, 1 woman, 9.4ms\n",
      "image 40/402 //workspace/dataset/test/images/11630_jpg.rf.99b0188949606bf8d57fcfaea94c6c15_orig.jpg: 640x640 1 girl, 9.3ms\n",
      "image 41/402 //workspace/dataset/test/images/11632_jpg.rf.d36db98136cbdd36a8e1b206a9f508ce_orig.jpg: 640x640 1 boy, 1 woman, 9.3ms\n",
      "image 42/402 //workspace/dataset/test/images/11754_jpg.rf.6a694f40960a834cc4b222e9879b0402_orig.jpg: 640x640 1 woman, 9.3ms\n",
      "image 43/402 //workspace/dataset/test/images/118_jpg.rf.41e96526344f0115d0c9232fd4f8ea17_orig.jpg: 640x640 1 man, 1 woman, 9.2ms\n",
      "image 44/402 //workspace/dataset/test/images/11931_jpg.rf.71f6307bc139cc3405df9e9911cdf400_orig.jpg: 640x640 1 boy, 1 man, 1 woman, 9.2ms\n",
      "image 45/402 //workspace/dataset/test/images/12039_jpg.rf.3b626fe66d99fc87bf77fd0f93b0ae94_orig.jpg: 640x640 1 girl, 9.3ms\n",
      "image 46/402 //workspace/dataset/test/images/12098_jpg.rf.e3bf81c3a88bee39414a8c7c1fe8a55e_orig.jpg: 640x640 1 boy, 1 woman, 9.3ms\n",
      "image 47/402 //workspace/dataset/test/images/120_jpg.rf.7b2856b113acd8f0f891939eaeeee12c_orig.jpg: 640x640 1 girl, 9.2ms\n",
      "image 48/402 //workspace/dataset/test/images/12186_jpg.rf.3b2ed153923dec7ca1e54203627baa15_orig.jpg: 640x640 1 girl, 13.3ms\n",
      "image 49/402 //workspace/dataset/test/images/12339_jpg.rf.7ac47b4a4c31e1e7dba3bab8fac446b0_orig.jpg: 640x640 1 boy, 12.0ms\n",
      "image 50/402 //workspace/dataset/test/images/123_jpg.rf.b4561dd2f93208ecc6d2cef64c64ef33_orig.jpg: 640x640 1 boy, 1 girl, 12.0ms\n",
      "image 51/402 //workspace/dataset/test/images/125_jpg.rf.4c2b45220f0425cf9c2510d4ef5476d0_orig.jpg: 640x640 1 girl, 12.0ms\n",
      "image 52/402 //workspace/dataset/test/images/12762_jpg.rf.688e096046c70f705e8ef1a813fdd529_orig.jpg: 640x640 1 girl, 1 woman, 11.6ms\n",
      "image 53/402 //workspace/dataset/test/images/127_jpg.rf.ec95c15254e75033eab59fe937cee885_orig.jpg: 640x640 1 boy, 11.6ms\n",
      "image 54/402 //workspace/dataset/test/images/12832_jpg.rf.288e79fa5347f0fdfb7513d686b84d6b_orig.jpg: 640x640 1 girl, 1 man, 11.6ms\n",
      "image 55/402 //workspace/dataset/test/images/12916_jpg.rf.adbf65f0d4bff51aefa6e516e21ae19a_orig.jpg: 640x640 (no detections), 13.1ms\n",
      "image 56/402 //workspace/dataset/test/images/12979_jpg.rf.e434f3aa72db8e4002d39700d53e2c18_orig.jpg: 640x640 1 girl, 12.6ms\n",
      "image 57/402 //workspace/dataset/test/images/12_jpg.rf.1fccd231ec9f9ed1086e807d10d86608_orig.jpg: 640x640 1 boy, 1 girl, 13.7ms\n",
      "image 58/402 //workspace/dataset/test/images/13008_jpg.rf.bf32290e29bffc863223164d027fe009_orig.jpg: 640x640 1 boy, 1 woman, 14.4ms\n",
      "image 59/402 //workspace/dataset/test/images/13075_jpg.rf.600b217ba95e7848eadd928142b3293e_orig.jpg: 640x640 1 girl, 1 woman, 13.8ms\n",
      "image 60/402 //workspace/dataset/test/images/13126_jpg.rf.c083bc861edb345c4057348b97e8d56b_orig.jpg: 640x640 1 girl, 3 womans, 14.1ms\n",
      "image 61/402 //workspace/dataset/test/images/13164_jpg.rf.cf4b8cf17359e710e63f9b20fb819836_orig.jpg: 640x640 1 boy, 1 man, 14.5ms\n",
      "image 62/402 //workspace/dataset/test/images/13182_jpg.rf.2c7b98afef5242cfed0e26bd2694a161_orig.jpg: 640x640 1 boy, 13.0ms\n",
      "image 63/402 //workspace/dataset/test/images/133_jpg.rf.4e52d0b03034ca779c1d2c41921ef8ef_orig.jpg: 640x640 1 girl, 13.0ms\n",
      "image 64/402 //workspace/dataset/test/images/13494_jpg.rf.1d3d61d53415059a102bf8291251f562_orig.jpg: 640x640 1 boy, 1 girl, 13.0ms\n",
      "image 65/402 //workspace/dataset/test/images/134_PNG_jpg.rf.b893904464b862933601a76552b62234_orig.jpg: 640x640 (no detections), 13.0ms\n",
      "image 66/402 //workspace/dataset/test/images/13574_jpg.rf.d19544f7ff195421846316a11f95c889_orig.jpg: 640x640 1 girl, 12.9ms\n",
      "image 67/402 //workspace/dataset/test/images/13666_jpg.rf.bd03eecb8073e369172e49d4f700cd40_orig.jpg: 640x640 1 woman, 12.5ms\n",
      "image 68/402 //workspace/dataset/test/images/136_jpg.rf.3575179b1c429153f242c93e3bcc4e57_orig.jpg: 640x640 1 boy, 12.5ms\n",
      "image 69/402 //workspace/dataset/test/images/13871_jpg.rf.3c446712c242ffda98fe5702f4d162be_orig.jpg: 640x640 2 boys, 19.0ms\n",
      "image 70/402 //workspace/dataset/test/images/14029_jpg.rf.234bf450587b6757bae9c0c5a066511f_orig.jpg: 640x640 1 boy, 1 woman, 18.2ms\n",
      "image 71/402 //workspace/dataset/test/images/14063_jpg.rf.54f84762a20c27d25852a1298f7b3997_orig.jpg: 640x640 1 woman, 19.3ms\n",
      "image 72/402 //workspace/dataset/test/images/14074_jpg.rf.60366473cf56b618bc3a57a39a51dd27_orig.jpg: 640x640 1 girl, 1 woman, 19.4ms\n",
      "image 73/402 //workspace/dataset/test/images/140_jpg.rf.39e0684323b36070257b84c46cca847a_orig.jpg: 640x640 1 boy, 18.8ms\n",
      "image 74/402 //workspace/dataset/test/images/14144_jpg.rf.a3f3c3f25f6a30018fcf0bf9c16d3da7_orig.jpg: 640x640 1 girl, 1 woman, 15.1ms\n",
      "image 75/402 //workspace/dataset/test/images/14172_jpg.rf.de015ab7bcaf0069936eef280a12ceba_orig.jpg: 640x640 1 boy, 16.9ms\n",
      "image 76/402 //workspace/dataset/test/images/141_jpg.rf.905bd379f786c2b1fb048ab0977c6fa1_orig.jpg: 640x640 1 girl, 15.2ms\n",
      "image 77/402 //workspace/dataset/test/images/1427_person_jpg.rf.85629a14a8226379ed1c6d5891205a6c_orig.jpg: 640x640 1 boy, 1 man, 1 woman, 15.1ms\n",
      "image 78/402 //workspace/dataset/test/images/14423_jpg.rf.b55363424974f8141fefed5be15173b1_orig.jpg: 640x640 1 girl, 15.6ms\n",
      "image 79/402 //workspace/dataset/test/images/14486_jpg.rf.0d754f0536855e2f79cc3ac896267a88_orig.jpg: 640x640 2 girls, 21.0ms\n",
      "image 80/402 //workspace/dataset/test/images/145_jpg.rf.42c53cf5ac60dcaffb64adf3ca6f6b03_orig.jpg: 640x640 1 girl, 20.7ms\n",
      "image 81/402 //workspace/dataset/test/images/14645_jpg.rf.f2ffdfa06092818add28055dbfb9723a_orig.jpg: 640x640 2 boys, 2 mans, 17.6ms\n",
      "image 82/402 //workspace/dataset/test/images/14760_jpg.rf.1eb57c43aed0169d82d56514e2654cbd_orig.jpg: 640x640 1 boy, 1 girl, 1 woman, 16.8ms\n",
      "image 83/402 //workspace/dataset/test/images/14842_jpg.rf.f6d7d6c35dfe0314d806d9cf18025804_orig.jpg: 640x640 (no detections), 13.6ms\n",
      "image 84/402 //workspace/dataset/test/images/148_jpg.rf.7b751975ba171e7d9400806b51c7b501_orig.jpg: 640x640 1 boy, 13.7ms\n",
      "image 85/402 //workspace/dataset/test/images/15324_jpg.rf.5a88ad948d7fc8d2d5d92e95723afea7_orig.jpg: 640x640 1 boy, 1 girl, 15.7ms\n",
      "image 86/402 //workspace/dataset/test/images/155_jpg.rf.6c6eff976800d40c7e46721e3412c475_orig.jpg: 640x640 1 boy, 15.8ms\n",
      "image 87/402 //workspace/dataset/test/images/15769_jpg.rf.eae694b2bdddbb62825723264c08e948_orig.jpg: 640x640 1 boy, 1 girl, 1 woman, 15.4ms\n",
      "image 88/402 //workspace/dataset/test/images/15790_jpg.rf.4c489af82e479046981e7ba80d03b80e_orig.jpg: 640x640 1 boy, 17.5ms\n",
      "image 89/402 //workspace/dataset/test/images/157_jpg.rf.08364b9b24b5dece5d62447c955f1e20_orig.jpg: 640x640 1 girl, 14.3ms\n",
      "image 90/402 //workspace/dataset/test/images/15855_jpg.rf.cbf555391cacf0f537e76878b21bf480_orig.jpg: 640x640 1 boy, 13.7ms\n",
      "image 91/402 //workspace/dataset/test/images/15989_jpg.rf.792cd44cca137cabf60d7e3970738df9_orig.jpg: 640x640 2 girls, 1 woman, 20.6ms\n",
      "image 92/402 //workspace/dataset/test/images/159_PNG_jpg.rf.1ac76b4cacec095fdf3e0f0a473d32c1_orig.jpg: 640x640 1 girl, 1 woman, 22.5ms\n",
      "image 93/402 //workspace/dataset/test/images/16077_jpg.rf.7255bc9224b6ed8c01b8f6fbb0d0821f_orig.jpg: 640x640 1 girl, 20.8ms\n",
      "image 94/402 //workspace/dataset/test/images/161_jpg.rf.9fb713740a01b7c56fdcc8d346aaf0d9_orig.jpg: 640x640 1 girl, 20.8ms\n",
      "image 95/402 //workspace/dataset/test/images/16311_jpg.rf.229b0628ed756212b6571e4e1d54e151_orig.jpg: 640x640 1 boy, 1 girl, 1 woman, 20.9ms\n",
      "image 96/402 //workspace/dataset/test/images/16427_jpg.rf.3088838c9f0c033a9297f581c3417621_orig.jpg: 640x640 1 boy, 1 girl, 18.6ms\n",
      "image 97/402 //workspace/dataset/test/images/164_jpg.rf.75d6665c4a17b100e09cc420a19fb66e_orig.jpg: 640x640 1 girl, 19.1ms\n",
      "image 98/402 //workspace/dataset/test/images/16631_jpg.rf.e7db929f19d8c3bd4515b271e5cc1544_orig.jpg: 640x640 1 man, 1 woman, 20.1ms\n",
      "image 99/402 //workspace/dataset/test/images/166_jpg.rf.b22c3ce6b429c7648b8fe1131093f230_orig.jpg: 640x640 1 girl, 18.8ms\n",
      "image 100/402 //workspace/dataset/test/images/167_jpg.rf.9a9e39bd45437be34576f0317d023db7_orig.jpg: 640x640 3 girls, 15.9ms\n",
      "image 101/402 //workspace/dataset/test/images/169_jpg.rf.8cfe9f0bf5a2335ab843b1056624397c_orig.jpg: 640x640 2 girls, 14.3ms\n",
      "image 102/402 //workspace/dataset/test/images/16_jpg.rf.aa11f432b7d9d6a0701d933b1be2b308_orig.jpg: 640x640 1 boy, 1 man, 14.4ms\n",
      "image 103/402 //workspace/dataset/test/images/17039_jpg.rf.e1de51e68e93bf59e2c57978b92355e2_orig.jpg: 640x640 1 girl, 1 woman, 14.2ms\n",
      "image 104/402 //workspace/dataset/test/images/17179_jpg.rf.9f7c5b4cec71158075d06fb184e5f767_orig.jpg: 640x640 1 boy, 1 man, 1 woman, 22.9ms\n",
      "image 105/402 //workspace/dataset/test/images/172_jpg.rf.47fef96ca194db898530e313e85dbc09_orig.jpg: 640x640 1 girl, 24.0ms\n",
      "image 106/402 //workspace/dataset/test/images/17434_jpg.rf.39da0e927169e70860b69a771340c22a_orig.jpg: 640x640 1 girl, 18.2ms\n",
      "image 107/402 //workspace/dataset/test/images/17447_jpg.rf.5f55a44c143f8886802351cf53a6d803_orig.jpg: 640x640 2 womans, 17.8ms\n",
      "image 108/402 //workspace/dataset/test/images/174_jpg.rf.09d41e4a76eebcdd4dd5c68e35fc4aa7_orig.jpg: 640x640 1 girl, 18.2ms\n",
      "image 109/402 //workspace/dataset/test/images/17565_jpg.rf.9356e40a8280eb004b8a76a2450d3bec_orig.jpg: 640x640 1 boy, 1 girl, 1 woman, 17.9ms\n",
      "image 110/402 //workspace/dataset/test/images/178_jpg.rf.08129214f0ae0683e1c72ec6b581fc9a_orig.jpg: 640x640 1 boy, 1 girl, 14.3ms\n",
      "image 111/402 //workspace/dataset/test/images/179_PNG_jpg.rf.8b89ae26c19c94c0f98661b937cb341a_orig.jpg: 640x640 1 woman, 14.4ms\n",
      "image 112/402 //workspace/dataset/test/images/17_jpg.rf.cfdac27bda3fdd3d74c4387d1e014e76_orig.jpg: 640x640 1 boy, 14.4ms\n",
      "image 113/402 //workspace/dataset/test/images/182_jpg.rf.74ded58d4601b2f034488d1f4e01e450_orig.jpg: 640x640 1 boy, 1 girl, 17.0ms\n",
      "image 114/402 //workspace/dataset/test/images/18361_jpg.rf.adb6b50eeb87269e46152eeb90154bca_orig.jpg: 640x640 1 girl, 1 man, 1 woman, 16.3ms\n",
      "image 115/402 //workspace/dataset/test/images/18495_jpg.rf.5010184fa84bcd7c4ef05d1cab2e6b97_orig.jpg: 640x640 1 girl, 1 woman, 15.2ms\n",
      "image 116/402 //workspace/dataset/test/images/184_jpg.rf.dc1d784a72c4a22949cc16c8225ca4bf_orig.jpg: 640x640 1 girl, 14.5ms\n",
      "image 117/402 //workspace/dataset/test/images/18588_jpg.rf.56018364697a363d190c845e00b9a7ab_orig.jpg: 640x640 2 girls, 14.5ms\n",
      "image 118/402 //workspace/dataset/test/images/18827_jpg.rf.2726d33233e666e70fd1c1a10d5defab_orig.jpg: 640x640 1 girl, 1 woman, 14.5ms\n",
      "image 119/402 //workspace/dataset/test/images/189_jpg.rf.57da19b6d4804345656767f626bdd8aa_orig.jpg: 640x640 1 girl, 15.2ms\n",
      "image 120/402 //workspace/dataset/test/images/19093_jpg.rf.1bd9709e14fc1d66e3ea544840f1112f_orig.jpg: 640x640 2 womans, 14.6ms\n",
      "image 121/402 //workspace/dataset/test/images/190_jpg.rf.4b7de7e18f0c543d865dcea0393f4d09_orig.jpg: 640x640 1 boy, 13.9ms\n",
      "image 122/402 //workspace/dataset/test/images/19458_jpg.rf.c48991797fccda6e99ddc091aa3b4731_orig.jpg: 640x640 1 girl, 13.9ms\n",
      "image 123/402 //workspace/dataset/test/images/194_jpg.rf.590df39f39ffd0f810736896834642db_orig.jpg: 640x640 1 girl, 13.8ms\n",
      "image 124/402 //workspace/dataset/test/images/19_jpg.rf.c86e51a7cca1f0400d1e2283a3c2c876_orig.jpg: 640x640 1 girl, 13.3ms\n",
      "image 125/402 //workspace/dataset/test/images/1bf41c6931136703_jpg.rf.9ef33bc5751b806de0ae4fcfc368c6a3_orig.jpg: 640x640 1 girl, 1 woman, 13.1ms\n",
      "image 126/402 //workspace/dataset/test/images/1d001cf59ad5d86b_jpg.rf.2249a9bfe7a163e3a8438c6243db3c3e_orig.jpg: 640x640 1 boy, 1 girl, 14.6ms\n",
      "image 127/402 //workspace/dataset/test/images/2007_000733_jpg.rf.87436e3323c20e43c156f9d4a35d3616_orig.jpg: 640x640 1 boy, 1 girl, 14.4ms\n",
      "image 128/402 //workspace/dataset/test/images/2007_003118_jpg.rf.82ce81195e2f5d3ea16c7c84c1e9f299_orig.jpg: 640x640 1 boy, 1 girl, 17.4ms\n",
      "image 129/402 //workspace/dataset/test/images/2007_004112_jpg.rf.f88028ff95d897c60b7d4d3dda7dcb63_orig.jpg: 640x640 1 man, 2 womans, 22.1ms\n",
      "image 130/402 //workspace/dataset/test/images/2007_006864_jpg.rf.0cab66e329f3d40baab43ff470908e20_orig.jpg: 640x640 3 womans, 21.9ms\n",
      "image 131/402 //workspace/dataset/test/images/20087_jpg.rf.6659b0a7438b456cd113a4f10ab36281_orig.jpg: 640x640 2 mans, 18.9ms\n",
      "image 132/402 //workspace/dataset/test/images/2008_000599_jpg.rf.c50d46952ba50efbad0f0b1f65a8d43c_orig.jpg: 640x640 2 boys, 1 girl, 1 man, 1 woman, 15.8ms\n",
      "image 133/402 //workspace/dataset/test/images/2008_000924_jpg.rf.c1d3b9b8bb262f9a372b0710c7c01cec_orig.jpg: 640x640 1 boy, 1 man, 10.8ms\n",
      "image 134/402 //workspace/dataset/test/images/2008_000940_jpg.rf.d70a114c631460bbfa46e17e7452e9e9_orig.jpg: 640x640 1 boy, 1 girl, 10.7ms\n",
      "image 135/402 //workspace/dataset/test/images/2008_001808_jpg.rf.8dac0481c4b726d9af87ca9e8b43210d_orig.jpg: 640x640 1 boy, 10.6ms\n",
      "image 136/402 //workspace/dataset/test/images/2008_001980_jpg.rf.e502677d1219dd1df2ff48e044fa808f_orig.jpg: 640x640 2 boys, 10.5ms\n",
      "image 137/402 //workspace/dataset/test/images/2008_002533_jpg.rf.1ab461dfc1c4db50b1c646240ad5f3d3_orig.jpg: 640x640 2 boys, 10.6ms\n",
      "image 138/402 //workspace/dataset/test/images/2008_002801_jpg.rf.f6b2b21d7cde13cc48593c7869aac9ea_orig.jpg: 640x640 1 boy, 1 girl, 10.6ms\n",
      "image 139/402 //workspace/dataset/test/images/2010_002821_jpg.rf.30f9ca8509295fd9929c67bc1a996d49_orig.jpg: 640x640 1 boy, 1 girl, 14.4ms\n",
      "image 140/402 //workspace/dataset/test/images/2011_007119_jpg.rf.950cd72b62155bc1c3c33a19a51081d9_orig.jpg: 640x640 1 boy, 14.4ms\n",
      "image 141/402 //workspace/dataset/test/images/204_jpg.rf.27461dfe1d4f3ab10c9c188451cacfb3_orig.jpg: 640x640 1 boy, 13.1ms\n",
      "image 142/402 //workspace/dataset/test/images/21034_jpg.rf.fac3b60849a8a4ce0fb158582c0921d6_orig.jpg: 640x640 1 boy, 1 girl, 12.6ms\n",
      "image 143/402 //workspace/dataset/test/images/21494_jpg.rf.883775c365b62137e0ca35e42dc17d59_orig.jpg: 640x640 1 girl, 13.1ms\n",
      "image 144/402 //workspace/dataset/test/images/21626_jpg.rf.8fc21ac990c80738509dbb741a565c1e_orig.jpg: 640x640 1 girl, 1 man, 1 woman, 12.6ms\n",
      "image 145/402 //workspace/dataset/test/images/216_jpg.rf.95754f110c8b0b33e24403f0fe3431e4_orig.jpg: 640x640 1 girl, 14.4ms\n",
      "image 146/402 //workspace/dataset/test/images/22032_jpg.rf.db24eabfb6e38c47f6277a64d17f4b5e_orig.jpg: 640x640 1 girl, 14.6ms\n",
      "image 147/402 //workspace/dataset/test/images/22091_jpg.rf.64c53b1ce5fd625873ccad9549abca7d_orig.jpg: 640x640 1 boy, 21.1ms\n",
      "image 148/402 //workspace/dataset/test/images/220_jpg.rf.86cbc46eafad46851a88af5b93c8d1a5_orig.jpg: 640x640 1 girl, 22.5ms\n",
      "image 149/402 //workspace/dataset/test/images/22154_jpg.rf.f6be6634636461d2f76a2e3315c191d7_orig.jpg: 640x640 1 boy, 1 man, 18.9ms\n",
      "image 150/402 //workspace/dataset/test/images/22265_jpg.rf.039436c32c299f9e85ee6f2197fbd7e0_orig.jpg: 640x640 1 boy, 21.3ms\n",
      "image 151/402 //workspace/dataset/test/images/22337_jpg.rf.43861563c2493244b01c2b409fce8da9_orig.jpg: 640x640 2 girls, 1 woman, 17.3ms\n",
      "image 152/402 //workspace/dataset/test/images/224_jpg.rf.9bdb19e915b39f02bc6d04efae71973b_orig.jpg: 640x640 1 girl, 15.3ms\n",
      "image 153/402 //workspace/dataset/test/images/225_jpg.rf.f23681c994d8a13bb009ca042692f744_orig.jpg: 640x640 1 girl, 14.6ms\n",
      "image 154/402 //workspace/dataset/test/images/229_jpg.rf.a88deb127bb871ca22d2d571d292cc7d_orig.jpg: 640x640 1 boy, 14.4ms\n",
      "image 155/402 //workspace/dataset/test/images/22_jpg.rf.8673df07b5f737cd4fa314553ef1503d_orig.jpg: 640x640 1 girl, 1 woman, 16.4ms\n",
      "image 156/402 //workspace/dataset/test/images/237_jpg.rf.5f41317a75458872419c6782ed6b8825_orig.jpg: 640x640 2 girls, 17.1ms\n",
      "image 157/402 //workspace/dataset/test/images/23_jpg.rf.76bb202175bc23cedc99837e729b7c40_orig.jpg: 640x640 1 girl, 15.3ms\n",
      "image 158/402 //workspace/dataset/test/images/244_jpg.rf.43659c617864244547e5b6c6df6991e0_orig.jpg: 640x640 1 boy, 15.0ms\n",
      "image 159/402 //workspace/dataset/test/images/249_jpg.rf.adec01a2c4bddee85d865032d70c7265_orig.jpg: 640x640 1 girl, 13.1ms\n",
      "image 160/402 //workspace/dataset/test/images/251_jpg.rf.24fb2ed7b8c7c48076a3a98d8fe7aea3_orig.jpg: 640x640 1 girl, 13.1ms\n",
      "image 161/402 //workspace/dataset/test/images/252_jpg.rf.c9100d804aedff5a0feacbd50b38a5c9_orig.jpg: 640x640 1 boy, 1 girl, 12.0ms\n",
      "image 162/402 //workspace/dataset/test/images/259_jpg.rf.7212a40dab0825483561df118a8b070d_orig.jpg: 640x640 1 boy, 12.5ms\n",
      "image 163/402 //workspace/dataset/test/images/266_jpg.rf.2a82f64c1faab71356e28c0259efafae_orig.jpg: 640x640 1 boy, 12.1ms\n",
      "image 164/402 //workspace/dataset/test/images/267_jpg.rf.10f67464b9ca4de373bbeb5ba60b8b24_orig.jpg: 640x640 1 boy, 12.1ms\n",
      "image 165/402 //workspace/dataset/test/images/268_jpg.rf.ea609392267ed63861ca8d0f2084980a_orig.jpg: 640x640 1 girl, 12.1ms\n",
      "image 166/402 //workspace/dataset/test/images/272_jpg.rf.079582e3dab3adbb3f728faaad7168bf_orig.jpg: 640x640 1 boy, 1 girl, 12.1ms\n",
      "image 167/402 //workspace/dataset/test/images/277_jpg.rf.5c710452cf7d2be3b5bedbec7d0cc16b_orig.jpg: 640x640 1 boy, 15.8ms\n",
      "image 168/402 //workspace/dataset/test/images/285_jpg.rf.358f9e49b046326afefd6890b44b9755_orig.jpg: 640x640 1 boy, 14.5ms\n",
      "image 169/402 //workspace/dataset/test/images/28_jpg.rf.a5274ecc24e737e60c6dfb83c03f1750_orig.jpg: 640x640 1 boy, 13.1ms\n",
      "image 170/402 //workspace/dataset/test/images/28d09c711d073f8d_jpg.rf.e39226b8af1ee81674e1540faf3c4e1a_orig.jpg: 640x640 1 boy, 2 girls, 13.0ms\n",
      "image 171/402 //workspace/dataset/test/images/298_jpg.rf.7bbf1c1122f1eb989c2832139e2ff21a_orig.jpg: 640x640 1 boy, 1 girl, 22.5ms\n",
      "image 172/402 //workspace/dataset/test/images/299_jpg.rf.1f480d11f2269751ab4835434c650aa4_orig.jpg: 640x640 1 boy, 1 girl, 12.6ms\n",
      "image 173/402 //workspace/dataset/test/images/29_jpg.rf.43a36bd870825ba1bbd8b8db18382108_orig.jpg: 640x640 1 boy, 1 girl, 12.3ms\n",
      "image 174/402 //workspace/dataset/test/images/2KDJ6ETYNRGGJBCELDMVSBFPY4_jpg.rf.2befc8f93b2ab5dd0af1296142f4bd85_orig.jpg: 640x640 1 boy, 2 mans, 2 womans, 17.7ms\n",
      "image 175/402 //workspace/dataset/test/images/2_jpg.rf.c10dae2d21a1516e1d440c0aad5a2513_orig.jpg: 640x640 (no detections), 17.5ms\n",
      "image 176/402 //workspace/dataset/test/images/300_jpg.rf.2497e98c92953f18a43ea2cf4130bbf7_orig.jpg: 640x640 2 girls, 19.6ms\n",
      "image 177/402 //workspace/dataset/test/images/304_jpg.rf.106ba8fede4500ae4550a03d9c0a9078_orig.jpg: 640x640 2 boys, 4 girls, 19.4ms\n",
      "image 178/402 //workspace/dataset/test/images/305_jpg.rf.6b21a3dd78779878b4b9bcca495c918c_orig.jpg: 640x640 1 girl, 14.5ms\n",
      "image 179/402 //workspace/dataset/test/images/310_jpg.rf.d337235b84df9be71259e7a538b574d9_orig.jpg: 640x640 1 boy, 1 girl, 15.4ms\n",
      "image 180/402 //workspace/dataset/test/images/316_jpg.rf.2a70ce2d2d5372d33885907710e7b45e_orig.jpg: 640x640 1 girl, 16.0ms\n",
      "image 181/402 //workspace/dataset/test/images/318_jpg.rf.b85a6ea5bd8c5f307ee23b025bb75e67_orig.jpg: 640x640 1 boy, 15.1ms\n",
      "image 182/402 //workspace/dataset/test/images/319_jpg.rf.85415890a5875d8fb3fec86c407be6bd_orig.jpg: 640x640 1 girl, 19.5ms\n",
      "image 183/402 //workspace/dataset/test/images/326_jpg.rf.a6f6da9d1cdee2d193072266e667a887_orig.jpg: 640x640 1 girl, 16.8ms\n",
      "image 184/402 //workspace/dataset/test/images/330_jpg.rf.b155385daa7a0ea9b91652362380eeaa_orig.jpg: 640x640 1 boy, 15.1ms\n",
      "image 185/402 //workspace/dataset/test/images/331_PNG_jpg.rf.998f29b2f949ac3a11ab8ae3968c8d0b_orig.jpg: 640x640 1 woman, 15.2ms\n",
      "image 186/402 //workspace/dataset/test/images/333_PNG_jpg.rf.41bd24fe1822f375dd884479d4f3eee0_orig.jpg: 640x640 1 girl, 1 woman, 15.9ms\n",
      "image 187/402 //workspace/dataset/test/images/335_jpg.rf.1d250215d9e5d496e24ff6c0fd95a25c_orig.jpg: 640x640 1 boy, 27.8ms\n",
      "image 188/402 //workspace/dataset/test/images/342_jpg.rf.fa01f4b16be69c61c3ba29b409179d05_orig.jpg: 640x640 1 boy, 12.6ms\n",
      "image 189/402 //workspace/dataset/test/images/349_jpg.rf.91f76864796b6c7c432ad3dbe9fbc181_orig.jpg: 640x640 1 boy, 1 girl, 12.6ms\n",
      "image 190/402 //workspace/dataset/test/images/34_jpg.rf.89b28e87ac29eb0e6eaddd5059ca7eba_orig.jpg: 640x640 1 boy, 1 girl, 15.5ms\n",
      "image 191/402 //workspace/dataset/test/images/356_jpg.rf.9e216fd7b20d58ff3e481cc0fc145180_orig.jpg: 640x640 2 boys, 1 man, 14.4ms\n",
      "image 192/402 //workspace/dataset/test/images/35_jpg.rf.d3d575a04aa38d1f34ed95d914484d08_orig.jpg: 640x640 1 boy, 1 woman, 15.7ms\n",
      "image 193/402 //workspace/dataset/test/images/35e3216f0a500727_jpg.rf.6d4d650192cf9c70d285b6ee2a4802f3_orig.jpg: 640x640 1 girl, 14.1ms\n",
      "image 194/402 //workspace/dataset/test/images/362_PNG_jpg.rf.39c1b15c9e069874d725e950c3bd9436_orig.jpg: 640x640 1 girl, 1 man, 13.0ms\n",
      "image 195/402 //workspace/dataset/test/images/366_PNG_jpg.rf.fe4625ab471c1281c73b11b6ac3a4384_orig.jpg: 640x640 1 boy, 1 man, 13.0ms\n",
      "image 196/402 //workspace/dataset/test/images/366_jpg.rf.7b1d50ff9433edff906f74cd871d0a8f_orig.jpg: 640x640 1 boy, 2 girls, 12.5ms\n",
      "image 197/402 //workspace/dataset/test/images/369_jpg.rf.a1e4413673ab118d38076cd268f2a4a9_orig.jpg: 640x640 1 girl, 12.6ms\n",
      "image 198/402 //workspace/dataset/test/images/372_jpg.rf.88b07ee55f36c3fe74342456bd10a2e9_orig.jpg: 640x640 2 boys, 15.3ms\n",
      "image 199/402 //workspace/dataset/test/images/373_jpg.rf.316fb694c0d38fd9d5913798e8c57d2d_orig.jpg: 640x640 1 girl, 20.4ms\n",
      "image 200/402 //workspace/dataset/test/images/394_jpg.rf.ef99a7f219aa0bebc65f5dc1b296c47a_orig.jpg: 640x640 1 boy, 1 girl, 34.8ms\n",
      "image 201/402 //workspace/dataset/test/images/3b105c7c64424daf_jpg.rf.856589cff799aed3794a7f2509776ee2_orig.jpg: 640x640 6 boys, 2 girls, 1 man, 1 woman, 12.0ms\n",
      "image 202/402 //workspace/dataset/test/images/3efecc09f37c203e_jpg.rf.1fae7c961dbdf58710277139d36de093_orig.jpg: 640x640 2 boys, 2 girls, 1 man, 12.1ms\n",
      "image 203/402 //workspace/dataset/test/images/405_PNG_jpg.rf.c779012b13fd2efd7f2d41d411aa52a8_orig.jpg: 640x640 1 girl, 1 woman, 12.5ms\n",
      "image 204/402 //workspace/dataset/test/images/413_PNG_jpg.rf.68a91ef989a7d2589946ec4dc4a30b92_orig.jpg: 640x640 2 boys, 1 girl, 12.1ms\n",
      "image 205/402 //workspace/dataset/test/images/428_PNG_jpg.rf.4f5bfedb4c863552fdcecab5eb4575d8_orig.jpg: 640x640 2 boys, 1 man, 1 woman, 14.9ms\n",
      "image 206/402 //workspace/dataset/test/images/451_PNG_jpg.rf.e17997eb2a2427bd953e45498d0e613d_orig.jpg: 640x640 1 girl, 2 mans, 16.6ms\n",
      "image 207/402 //workspace/dataset/test/images/45_jpg.rf.0492f72d0b8427127e84f2c57e133009_orig.jpg: 640x640 1 boy, 17.0ms\n",
      "image 208/402 //workspace/dataset/test/images/490_PNG_jpg.rf.d0e91f3a4f29270c749672f11e5f0cfa_orig.jpg: 640x640 1 boy, 1 woman, 17.5ms\n",
      "image 209/402 //workspace/dataset/test/images/4_jpg.rf.04559ed0b0efa7d36ee794b7a07de502_orig.jpg: 640x640 1 boy, 16.3ms\n",
      "image 210/402 //workspace/dataset/test/images/50_jpg.rf.cbeca5a4f59298a72f4e026642a5fdfa_orig.jpg: 640x640 1 girl, 15.2ms\n",
      "image 211/402 //workspace/dataset/test/images/524632c51dd1908c_jpg.rf.f64422b7445b10147c5e672b5048ec77_orig.jpg: 640x640 1 boy, 15.1ms\n",
      "image 212/402 //workspace/dataset/test/images/52_jpg.rf.1d250c53ebf025563548a46c2f548365_orig.jpg: 640x640 1 girl, 31.0ms\n",
      "image 213/402 //workspace/dataset/test/images/556efe81ca77aca2_jpg.rf.16a357403623b0d78553ccc1ba7413c3_orig.jpg: 640x640 1 girl, 18.1ms\n",
      "image 214/402 //workspace/dataset/test/images/56_jpg.rf.fcb718c55be0bf3939230d1abcd7c4bb_orig.jpg: 640x640 1 girl, 1 woman, 20.5ms\n",
      "image 215/402 //workspace/dataset/test/images/58_jpg.rf.fefcfe8b8b2b445a1373094b65e98e99_orig.jpg: 640x640 1 boy, 17.7ms\n",
      "image 216/402 //workspace/dataset/test/images/5c29d4b0a3188bbf_jpg.rf.5299d7241ff0d1b44a2829540b7d4799_orig.jpg: 640x640 1 woman, 18.8ms\n",
      "image 217/402 //workspace/dataset/test/images/60_jpg.rf.cbed78e32b47419cb69862d328db22f1_orig.jpg: 640x640 2 boys, 16.6ms\n",
      "image 218/402 //workspace/dataset/test/images/69_jpg.rf.8e01ac9340d3e389521ba06644fa2062_orig.jpg: 640x640 1 boy, 16.7ms\n",
      "image 219/402 //workspace/dataset/test/images/6_jpg.rf.a4689b5c7e2dc42c1c6ad9baacca2a2c_orig.jpg: 640x640 1 boy, 1 girl, 16.8ms\n",
      "image 220/402 //workspace/dataset/test/images/735c08a7b6f0de3b_jpg.rf.b697f5a5384a34631e7c60bd08314c33_orig.jpg: 640x640 1 boy, 16.6ms\n",
      "image 221/402 //workspace/dataset/test/images/74_jpg.rf.c90a752759a336937650dc207c375f1e_orig.jpg: 640x640 1 boy, 1 girl, 16.6ms\n",
      "image 222/402 //workspace/dataset/test/images/79_jpg.rf.362517985aebec2daa660711b446c67c_orig.jpg: 640x640 1 girl, 16.9ms\n",
      "image 223/402 //workspace/dataset/test/images/7ce788c2be898af3_jpg.rf.701481a5aeac13b78ce9057fd4eeb31f_orig.jpg: 640x640 7 boys, 1 girl, 2 mans, 12.9ms\n",
      "image 224/402 //workspace/dataset/test/images/84_jpg.rf.e3fa792ecde6c0f4a9fc0af64bcf9e15_orig.jpg: 640x640 1 boy, 1 girl, 9.8ms\n",
      "image 225/402 //workspace/dataset/test/images/85_jpg.rf.1bf9d84a04746a64c815d22417ed88ca_orig.jpg: 640x640 1 girl, 9.9ms\n",
      "image 226/402 //workspace/dataset/test/images/87_jpg.rf.d242ae01b95c65274e3e1b4dfb7479fa_orig.jpg: 640x640 1 girl, 9.9ms\n",
      "image 227/402 //workspace/dataset/test/images/94_jpg.rf.d9a6b24163bf0c715d8d3ea976f673f2_orig.jpg: 640x640 1 boy, 1 girl, 9.8ms\n",
      "image 228/402 //workspace/dataset/test/images/9aed2de1ad716dec_jpg.rf.fffd132000c99b7b68a8b5ab6af20ad0_orig.jpg: 640x640 2 boys, 9.8ms\n",
      "image 229/402 //workspace/dataset/test/images/Capture-51_PNG_jpg.rf.396d607b39cc1335001abb7b8020bb71_orig.jpg: 640x640 1 boy, 2 womans, 9.8ms\n",
      "image 230/402 //workspace/dataset/test/images/Capture-63_PNG_jpg.rf.dedea2454a73c3da5877683997e4c684_orig.jpg: 640x640 1 girl, 1 man, 11.3ms\n",
      "image 231/402 //workspace/dataset/test/images/Download-1-_mp4-0006_jpg.rf.de9583095fd8d2b176c08915f34f6f44_orig.jpg: 640x640 1 girl, 10.8ms\n",
      "image 232/402 //workspace/dataset/test/images/Download-1-_mp4-0010_jpg.rf.2d53f4c862a563d73c44b3b5539ef7d9_orig.jpg: 640x640 1 man, 1 woman, 10.9ms\n",
      "image 233/402 //workspace/dataset/test/images/Download-3-_mp4-0002_jpg.rf.ea0bf40722bfdb756c553f686d518dd6_orig.jpg: 640x640 1 woman, 10.8ms\n",
      "image 234/402 //workspace/dataset/test/images/Snaptik_app_7350359736610589994_mp4-0005_jpg.rf.d7c44871f4b2720bccc7889d227de066_orig.jpg: 640x640 1 boy, 1 woman, 14.8ms\n",
      "image 235/402 //workspace/dataset/test/images/Snaptik_app_7353018637487263007_mp4-0000_jpg.rf.836b886bb316e130974678653cfb9d81_orig.jpg: 640x640 2 girls, 1 man, 12.2ms\n",
      "image 236/402 //workspace/dataset/test/images/WIN_20240425_12_45_20_Pro_jpg.rf.e40246ef596ec175d9131295563aca85_orig.jpg: 640x640 1 girl, 12.1ms\n",
      "image 237/402 //workspace/dataset/test/images/WIN_20240425_12_48_23_Pro_jpg.rf.923cddc2d3a6676be8d7071e2b362425_orig.jpg: 640x640 1 girl, 1 woman, 11.6ms\n",
      "image 238/402 //workspace/dataset/test/images/download-19-_jpg.rf.b018cf7ce16510c03ba3a0bdb923daa6_orig.jpg: 640x640 2 boys, 2 girls, 1 man, 2 womans, 13.5ms\n",
      "image 239/402 //workspace/dataset/test/images/download-20-_jpg.rf.19ed1a28182dd8f370fc84c01b452d45_orig.jpg: 640x640 2 boys, 4 mans, 1 woman, 12.0ms\n",
      "image 240/402 //workspace/dataset/test/images/download-23-_jpg.rf.335263f0c34aca474a47d9036e935ca5_orig.jpg: 640x640 3 girls, 12.3ms\n",
      "image 241/402 //workspace/dataset/test/images/download-25-_jpg.rf.a2b8856bbefd13589feaa7b2c6d43b40_orig.jpg: 640x640 6 boys, 11.7ms\n",
      "image 242/402 //workspace/dataset/test/images/download-26-_jpg.rf.8db5e98de1bba49ff7b743dd60fde6ee_orig.jpg: 640x640 5 girls, 11.7ms\n",
      "image 243/402 //workspace/dataset/test/images/download-27-_jpg.rf.17291aacd279a12476e6058f2a1587af_orig.jpg: 640x640 1 boy, 2 girls, 1 woman, 11.7ms\n",
      "image 244/402 //workspace/dataset/test/images/download-29-_jpg.rf.d083e1309974183883039c9d864ccadd_orig.jpg: 640x640 1 boy, 2 girls, 11.7ms\n",
      "image 245/402 //workspace/dataset/test/images/download-33-_jpg.rf.0d7de6a4c00b813d10b8a4054ba66077_orig.jpg: 640x640 1 girl, 13.9ms\n",
      "image 246/402 //workspace/dataset/test/images/download-36-_jpg.rf.0a1d06f25cfa1efc369f55adc0c97c50_orig.jpg: 640x640 3 boys, 12.1ms\n",
      "image 247/402 //workspace/dataset/test/images/download-4-_jpg.rf.57551acb1abcbc403d9d8a46caba1fc4_orig.jpg: 640x640 7 boys, 2 girls, 12.1ms\n",
      "image 248/402 //workspace/dataset/test/images/download-41-_jpg.rf.1c293e7655ff445bfe025e9fdb7060b0_orig.jpg: 640x640 1 boy, 2 girls, 1 woman, 12.1ms\n",
      "image 249/402 //workspace/dataset/test/images/download-43-_jpg.rf.0e7c889a31a0d87fdfb17648aa3b5a77_orig.jpg: 640x640 1 girl, 6 womans, 12.2ms\n",
      "image 250/402 //workspace/dataset/test/images/download-47-_jpg.rf.e697f80d7eef7b62cb06b5defa901326_orig.jpg: 640x640 5 womans, 12.2ms\n",
      "image 251/402 //workspace/dataset/test/images/download-6-_jpg.rf.1ca3b1c0dc61a9a15bb68a6210d2455a_orig.jpg: 640x640 2 girls, 13.7ms\n",
      "image 252/402 //workspace/dataset/test/images/download-7-_jpg.rf.b27331040a7adc22fe3ba2b80aa3ed31_orig.jpg: 640x640 1 boy, 2 girls, 13.7ms\n",
      "image 253/402 //workspace/dataset/test/images/download-77-_jpg.rf.15fabee1845b23333a8b7613ee2b29bb_orig.jpg: 640x640 1 girl, 10 womans, 20.5ms\n",
      "image 254/402 //workspace/dataset/test/images/download-8-_jpg.rf.902121f8ec153f93892b03320c56750e_orig.jpg: 640x640 2 boys, 2 girls, 1 man, 1 woman, 18.1ms\n",
      "image 255/402 //workspace/dataset/test/images/frame_0_png_jpg.rf.0c3f2880df2221869a98ab18341a3e4c_orig.jpg: 640x640 1 boy, 17.1ms\n",
      "image 256/402 //workspace/dataset/test/images/frame_0_png_jpg.rf.878d43d8293e91a43efa92656e6501aa_orig.jpg: 640x640 (no detections), 16.8ms\n",
      "image 257/402 //workspace/dataset/test/images/frame_0_png_jpg.rf.aef998822cee41142aef526be3cf53bb_orig.jpg: 640x640 1 boy, 1 woman, 14.5ms\n",
      "image 258/402 //workspace/dataset/test/images/frame_0_png_jpg.rf.b31636a7aeb6c657428a7d9c951abe5c_orig.jpg: 640x640 1 boy, 1 man, 1 woman, 16.4ms\n",
      "image 259/402 //workspace/dataset/test/images/frame_0_png_jpg.rf.b8e6f448a0412b06247c3d54c694d66a_orig.jpg: 640x640 1 boy, 1 man, 1 woman, 16.8ms\n",
      "image 260/402 //workspace/dataset/test/images/frame_0_png_jpg.rf.e88b90bd33d9aad370dc29e3a8486627_orig.jpg: 640x640 2 boys, 1 girl, 1 man, 1 woman, 15.2ms\n",
      "image 261/402 //workspace/dataset/test/images/frame_1000_png_jpg.rf.ad209a17ad4a13bc23232d1d9041203c_orig.jpg: 640x640 4 boys, 2 mans, 15.9ms\n",
      "image 262/402 //workspace/dataset/test/images/frame_1020_png_jpg.rf.58e0754d1249ec7e41214e0729680134_orig.jpg: 640x640 1 boy, 1 girl, 1 man, 1 woman, 21.5ms\n",
      "image 263/402 //workspace/dataset/test/images/frame_1050_png_jpg.rf.41b7fa9cbb0b005400d5b0247ca70831_orig.jpg: 640x640 2 boys, 23.8ms\n",
      "image 264/402 //workspace/dataset/test/images/frame_1050_png_jpg.rf.69f75c5f359acfe8600fd1cf082efd92_orig.jpg: 640x640 1 boy, 20.6ms\n",
      "image 265/402 //workspace/dataset/test/images/frame_1050_png_jpg.rf.d963a84d6ce870dce2f68f353b6208e3_orig.jpg: 640x640 2 boys, 1 man, 16.0ms\n",
      "image 266/402 //workspace/dataset/test/images/frame_1050_png_jpg.rf.feadebc61545bd6a5753bce8ed6f3419_orig.jpg: 640x640 1 girl, 18.6ms\n",
      "image 267/402 //workspace/dataset/test/images/frame_1080_png_jpg.rf.3d31c2ed39a1cd410b1741a33030de8e_orig.jpg: 640x640 1 girl, 1 woman, 19.0ms\n",
      "image 268/402 //workspace/dataset/test/images/frame_11225_png_jpg.rf.d9c2f5b7eb2efc490d7b4ff7c60211b7_orig.jpg: 640x640 1 boy, 1 girl, 15.3ms\n",
      "image 269/402 //workspace/dataset/test/images/frame_1168_png_jpg.rf.1a7f92b26b46502c422cc2ab55c888e1_orig.jpg: 640x640 1 girl, 18.6ms\n",
      "image 270/402 //workspace/dataset/test/images/frame_1196_png_jpg.rf.96c58841800944436054596d3d1c196f_orig.jpg: 640x640 1 boy, 16.7ms\n",
      "image 271/402 //workspace/dataset/test/images/frame_1200_png_jpg.rf.356d4f2920d519ab7c8edb0f92527aa3_orig.jpg: 640x640 1 boy, 15.0ms\n",
      "image 272/402 //workspace/dataset/test/images/frame_120_png_jpg.rf.66f4b8e5aad90f5949ffc756b533157f_orig.jpg: 640x640 1 man, 1 woman, 14.8ms\n",
      "image 273/402 //workspace/dataset/test/images/frame_122_png_jpg.rf.7f325ee127e1d240780c7c87b03075ad_orig.jpg: 640x640 3 mans, 13.2ms\n",
      "image 274/402 //workspace/dataset/test/images/frame_12572_png_jpg.rf.fb187c734947c1c3458e0fde95a7e51a_orig.jpg: 640x640 2 girls, 12.6ms\n",
      "image 275/402 //workspace/dataset/test/images/frame_125_png_jpg.rf.e7f8f7c558a7fac63ced5b264ef14477_orig.jpg: 640x640 1 boy, 1 man, 12.6ms\n",
      "image 276/402 //workspace/dataset/test/images/frame_12750_png_jpg.rf.3dfa35f4c8c94340c68d434d9a936f18_orig.jpg: 640x640 1 girl, 12.5ms\n",
      "image 277/402 //workspace/dataset/test/images/frame_1341_png_jpg.rf.1ac5b558ef839a3c388166ddfe995a42_orig.jpg: 640x640 1 girl, 12.2ms\n",
      "image 278/402 //workspace/dataset/test/images/frame_1350_png_jpg.rf.d4ed58f0154f981909117a96e2401792_orig.jpg: 640x640 1 boy, 1 man, 13.6ms\n",
      "image 279/402 //workspace/dataset/test/images/frame_1380_png_jpg.rf.cb4213e2ff47a52eb2d742730b9039bd_orig.jpg: 640x640 1 girl, 13.1ms\n",
      "image 280/402 //workspace/dataset/test/images/frame_14250_png_jpg.rf.75392195b30ee7ed4f18a8e38e55e744_orig.jpg: 640x640 1 girl, 12.5ms\n",
      "image 281/402 //workspace/dataset/test/images/frame_145_png_jpg.rf.a2b945dfd73ad9ea15a61f1fb1c9374e_orig.jpg: 640x640 1 woman, 12.5ms\n",
      "image 282/402 //workspace/dataset/test/images/frame_14817_png_jpg.rf.9b3a1883225d7c220979c3777d7ebe7b_orig.jpg: 640x640 2 boys, 1 woman, 12.5ms\n",
      "image 283/402 //workspace/dataset/test/images/frame_1490_png_jpg.rf.938b1ed0d03b1f98a6f87e67cc9edf2f_orig.jpg: 640x640 1 boy, 1 girl, 1 woman, 11.9ms\n",
      "image 284/402 //workspace/dataset/test/images/frame_15000_png_jpg.rf.6f9b2760a024aa36b476aaea04af38eb_orig.jpg: 640x640 1 girl, 11.5ms\n",
      "image 285/402 //workspace/dataset/test/images/frame_1500_png_jpg.rf.7394863df2a9e9d70077504aae9172a5_orig.jpg: 640x640 1 man, 11.6ms\n",
      "image 286/402 //workspace/dataset/test/images/frame_1500_png_jpg.rf.8c7daa9b115bdb65e5ee488ab90df70e_orig.jpg: 640x640 (no detections), 11.6ms\n",
      "image 287/402 //workspace/dataset/test/images/frame_150_png_jpg.rf.61fceb6fa18ed50b8daccbdfa2d87156_orig.jpg: 640x640 1 boy, 1 man, 11.6ms\n",
      "image 288/402 //workspace/dataset/test/images/frame_150_png_jpg.rf.e25dd419239b183ec3d2378d73adc542_orig.jpg: 640x640 1 boy, 11.6ms\n",
      "image 289/402 //workspace/dataset/test/images/frame_150_png_jpg.rf.f772906eeb9b4884ee9cc2ea1c8e953c_orig.jpg: 640x640 3 boys, 2 girls, 11.6ms\n",
      "image 290/402 //workspace/dataset/test/images/frame_15715_png_jpg.rf.054ce9d77356208d333793d119edee64_orig.jpg: 640x640 1 boy, 11.6ms\n",
      "image 291/402 //workspace/dataset/test/images/frame_15900_png_jpg.rf.502dc9fe16d46015a7fe56453ef00a6a_orig.jpg: 640x640 1 girl, 13.1ms\n",
      "image 292/402 //workspace/dataset/test/images/frame_1650_png_jpg.rf.3c1207b5535d382649a54b214d708c95_orig.jpg: 640x640 2 boys, 2 girls, 14.3ms\n",
      "image 293/402 //workspace/dataset/test/images/frame_1650_png_jpg.rf.8f2cfcb6c279c44f67f4f0267ce77f0b_orig.jpg: 640x640 1 boy, 1 man, 15.5ms\n",
      "image 294/402 //workspace/dataset/test/images/frame_1752_png_jpg.rf.f3c886fd7e6be72731e522ddeb553507_orig.jpg: 640x640 1 girl, 2 mans, 15.1ms\n",
      "image 295/402 //workspace/dataset/test/images/frame_18000_png_jpg.rf.5403deabd3b1bea57e7d01ff41a00599_orig.jpg: 640x640 1 girl, 15.2ms\n",
      "image 296/402 //workspace/dataset/test/images/frame_1800_png_jpg.rf.388d835216a451c8d0707962b0f496eb_orig.jpg: 640x640 1 girl, 14.5ms\n",
      "image 297/402 //workspace/dataset/test/images/frame_1800_png_jpg.rf.ba65b7457831969ae5f49ae6d92d8aba_orig.jpg: 640x640 1 boy, 15.0ms\n",
      "image 298/402 //workspace/dataset/test/images/frame_1800_png_jpg.rf.f6a21251c2a5bc4a07cef13211512e34_orig.jpg: 640x640 1 boy, 1 man, 1 woman, 14.3ms\n",
      "image 299/402 //workspace/dataset/test/images/frame_1800_png_jpg.rf.fdb52c55ce130ca833a59e4658b5b3bc_orig.jpg: 640x640 3 boys, 1 man, 15.4ms\n",
      "image 300/402 //workspace/dataset/test/images/frame_180_png_jpg.rf.6eb752447011adf306dd5f60a9e99a0c_orig.jpg: 640x640 1 woman, 13.1ms\n",
      "image 301/402 //workspace/dataset/test/images/frame_180_png_jpg.rf.f1f537519011e1690c4ccd9a00207e82_orig.jpg: 640x640 1 man, 13.0ms\n",
      "image 302/402 //workspace/dataset/test/images/frame_1950_png_jpg.rf.10c234da50e5200f02a0733a72a79a91_orig.jpg: 640x640 1 boy, 1 woman, 12.6ms\n",
      "image 303/402 //workspace/dataset/test/images/frame_1950_png_jpg.rf.c92887706407af4a6be4179120c6c5eb_orig.jpg: 640x640 1 girl, 12.6ms\n",
      "image 304/402 //workspace/dataset/test/images/frame_2400_png_jpg.rf.10f63e92a70561b4d9420471a0ca9393_orig.jpg: 640x640 1 boy, 12.7ms\n",
      "image 305/402 //workspace/dataset/test/images/frame_2400_png_jpg.rf.b2b14c5de8154406a889a0d01f3e5705_orig.jpg: 640x640 1 boy, 12.7ms\n",
      "image 306/402 //workspace/dataset/test/images/frame_250_png_jpg.rf.9acc9e85fa5fba8a405b42c36b3aa8fb_orig.jpg: 640x640 (no detections), 12.5ms\n",
      "image 307/402 //workspace/dataset/test/images/frame_2550_png_jpg.rf.126572482511a85b981fc06b85beee57_orig.jpg: 640x640 1 boy, 1 girl, 1 man, 13.6ms\n",
      "image 308/402 //workspace/dataset/test/images/frame_25650_png_jpg.rf.1459f92e977225465c46dbe3fbdf6489_orig.jpg: 640x640 1 girl, 13.6ms\n",
      "image 309/402 //workspace/dataset/test/images/frame_270_png_jpg.rf.fb6f95f1c1e6a54d3f57932589f8b66d_orig.jpg: 640x640 2 mans, 13.0ms\n",
      "image 310/402 //workspace/dataset/test/images/frame_2831_png_jpg.rf.0bc95f77564640c2496b04493b24805d_orig.jpg: 640x640 1 boy, 1 girl, 1 man, 1 woman, 13.1ms\n",
      "image 311/402 //workspace/dataset/test/images/frame_290_png_jpg.rf.91e0f71e40dd752026399f1f2db1ab7a_orig.jpg: 640x640 1 boy, 1 man, 13.1ms\n",
      "image 312/402 //workspace/dataset/test/images/frame_298_png_jpg.rf.09fc7f142d3804413bead7f6aa8fc590_orig.jpg: 640x640 1 boy, 1 man, 2 womans, 13.0ms\n",
      "image 313/402 //workspace/dataset/test/images/frame_298_png_jpg.rf.1c2cc4edd1a905323ec760ada3625dbe_orig.jpg: 640x640 1 boy, 11.9ms\n",
      "image 314/402 //workspace/dataset/test/images/frame_298_png_jpg.rf.f5b9e3264e1fe8ddb6b7d8f31b3a95db_orig.jpg: 640x640 1 boy, 1 man, 12.0ms\n",
      "image 315/402 //workspace/dataset/test/images/image-102-_jpg.rf.86f2be1c454c186b2d6e8d1b65600f64_orig.jpg: 640x640 2 girls, 14.2ms\n",
      "image 316/402 //workspace/dataset/test/images/image-113-_jpg.rf.47cbe76bbbb232d0e9b755be39ad04ec_orig.jpg: 640x640 1 boy, 1 man, 16.4ms\n",
      "image 317/402 //workspace/dataset/test/images/image-192-_jpg.rf.afc787ebd48edefc72f6aeb63ca7e38b_orig.jpg: 640x640 4 boys, 18.0ms\n",
      "image 318/402 //workspace/dataset/test/images/image-216-_jpg.rf.efd7d3d0f6ce15c5b14c8937d83f1bba_orig.jpg: 640x640 4 boys, 2 girls, 2 mans, 17.2ms\n",
      "image 319/402 //workspace/dataset/test/images/image-23-_jpg.rf.7eb9b7a205eabd63fbc2e80045658df7_orig.jpg: 640x640 2 boys, 14.3ms\n",
      "image 320/402 //workspace/dataset/test/images/image-27-_jpg.rf.ff4b508989018f4122745b2375477543_orig.jpg: 640x640 1 boy, 17.8ms\n",
      "image 321/402 //workspace/dataset/test/images/image-311-_jpg.rf.6295a135297ef03f8e5ab807d7d46d94_orig.jpg: 640x640 2 boys, 1 man, 19.0ms\n",
      "image 322/402 //workspace/dataset/test/images/image-317-_jpg.rf.99b8ec96abdf978eb1ff9b4c89f3cc2f_orig.jpg: 640x640 1 boy, 3 girls, 1 woman, 19.3ms\n",
      "image 323/402 //workspace/dataset/test/images/image-361-_jpg.rf.a0b9313640f224f1543ec08943bc590c_orig.jpg: 640x640 2 boys, 1 girl, 1 man, 19.3ms\n",
      "image 324/402 //workspace/dataset/test/images/image-386-_jpg.rf.753bdd9579fbcec4130c0c02289d860b_orig.jpg: 640x640 2 girls, 1 man, 1 woman, 18.5ms\n",
      "image 325/402 //workspace/dataset/test/images/image-387-_jpg.rf.9719b4e913d178e23742e6a68e22b9da_orig.jpg: 640x640 4 boys, 1 girl, 1 man, 13.9ms\n",
      "image 326/402 //workspace/dataset/test/images/image-394-_jpg.rf.93864f44b9f6b00aa16c2b5f959b54a2_orig.jpg: 640x640 2 boys, 1 man, 13.8ms\n",
      "image 327/402 //workspace/dataset/test/images/image-410-_jpg.rf.c0c056aee877e3902c403c14b8cba7f9_orig.jpg: 640x640 1 boy, 13.8ms\n",
      "image 328/402 //workspace/dataset/test/images/image-422-_jpg.rf.872319ce1edce4eb78077b11df25c267_orig.jpg: 640x640 1 boy, 1 girl, 15.3ms\n",
      "image 329/402 //workspace/dataset/test/images/image-46-_jpg.rf.71121077ae0a8343675b4e1d3e420feb_orig.jpg: 640x640 1 boy, 1 girl, 15.8ms\n",
      "image 330/402 //workspace/dataset/test/images/image-463-_jpg.rf.17960f63d2852efb17eb99c9085fa731_orig.jpg: 640x640 2 boys, 1 man, 1 woman, 17.3ms\n",
      "image 331/402 //workspace/dataset/test/images/image-47-_jpg.rf.6ecf0293c75ee1ccac9291d387314b7f_orig.jpg: 640x640 1 boy, 1 girl, 16.4ms\n",
      "image 332/402 //workspace/dataset/test/images/image-494-_jpg.rf.9e605316c3b12394b7800c050314a43e_orig.jpg: 640x640 1 girl, 1 man, 17.9ms\n",
      "image 333/402 //workspace/dataset/test/images/image-50-_jpg.rf.93cdd231757450a65fb40dde0935299b_orig.jpg: 640x640 1 boy, 14.5ms\n",
      "image 334/402 //workspace/dataset/test/images/image-58-_jpg.rf.a8401e5f839618cc7764e8caa42e156a_orig.jpg: 640x640 1 boy, 1 girl, 13.2ms\n",
      "image 335/402 //workspace/dataset/test/images/image-61-_jpg.rf.e76c501787e97f1ba0d1501edc1665c5_orig.jpg: 640x640 1 woman, 13.3ms\n",
      "image 336/402 //workspace/dataset/test/images/image-62-_jpg.rf.c7928b0127654dc84e53fd4c8db9d308_orig.jpg: 640x640 3 boys, 3 mans, 2 womans, 14.4ms\n",
      "image 337/402 //workspace/dataset/test/images/image-69-_jpg.rf.12006f070f2b38f1138e0147cc987bb8_orig.jpg: 640x640 1 girl, 13.3ms\n",
      "image 338/402 //workspace/dataset/test/images/image-77-_jpg.rf.8e8170650af065ba45ac4ac8d3cc7792_orig.jpg: 640x640 2 boys, 1 girl, 13.1ms\n",
      "image 339/402 //workspace/dataset/test/images/image-84-_jpg.rf.8d91b5274c79a647cfe8f509fb9efca4_orig.jpg: 640x640 1 boy, 3 girls, 1 man, 12.5ms\n",
      "image 340/402 //workspace/dataset/test/images/image-89-_jpg.rf.f79583269882bf1b8ec3290f94f77930_orig.jpg: 640x640 1 boy, 12.2ms\n",
      "image 341/402 //workspace/dataset/test/images/image-90-_jpg.rf.ef092bd9a4da45e69683dec534c5a2c1_orig.jpg: 640x640 1 boy, 2 mans, 1 woman, 12.1ms\n",
      "image 342/402 //workspace/dataset/test/images/images-13-_jpg.rf.82bca86b70867efee92d6df3d55f6da7_orig.jpg: 640x640 1 girl, 7 womans, 13.1ms\n",
      "image 343/402 //workspace/dataset/test/images/images-2025-06-10T102450_534_jpg.rf.d32a1c1d12304755320390d284691233_orig.jpg: 640x640 1 girl, 12.1ms\n",
      "image 344/402 //workspace/dataset/test/images/images-2025-06-10T102500_847_jpg.rf.d0eb7b70df2507a31e06676e6f166536_orig.jpg: 640x640 1 boy, 1 girl, 12.3ms\n",
      "image 345/402 //workspace/dataset/test/images/images-2025-06-10T102511_762_jpg.rf.0e43ac2f83559c72b8a14638f0dd0dea_orig.jpg: 640x640 1 boy, 3 girls, 2 mans, 15.4ms\n",
      "image 346/402 //workspace/dataset/test/images/images-2025-06-10T102525_373_jpg.rf.21acac52e66490fd9d9f15dd772fb5dd_orig.jpg: 640x640 1 boy, 13.8ms\n",
      "image 347/402 //workspace/dataset/test/images/images-2025-06-10T102602_127_jpg.rf.ff193a36cdbbc0a218484319d289bed6_orig.jpg: 640x640 2 boys, 3 girls, 12.6ms\n",
      "image 348/402 //workspace/dataset/test/images/images-2025-06-10T102610_598_jpg.rf.41ed3ca2a625d06a5787253a3b919bd4_orig.jpg: 640x640 2 boys, 1 girl, 12.7ms\n",
      "image 349/402 //workspace/dataset/test/images/images-2025-06-10T102616_120_jpg.rf.d62a2b5f8b84c72c6fe3a8ae79939d32_orig.jpg: 640x640 1 boy, 2 girls, 12.7ms\n",
      "image 350/402 //workspace/dataset/test/images/images-2025-06-10T102622_555_jpg.rf.c5e3d89f811e268648c0eb8beac3b8ab_orig.jpg: 640x640 4 boys, 1 girl, 16.9ms\n",
      "image 351/402 //workspace/dataset/test/images/images-2025-06-10T102654_713_jpg.rf.4599dee4269380533aaf1c9c11634257_orig.jpg: 640x640 1 girl, 1 woman, 12.7ms\n",
      "image 352/402 //workspace/dataset/test/images/images-2025-06-10T102717_103_jpg.rf.f51a3de79a9a9ab71aa514c6ee0d7647_orig.jpg: 640x640 1 boy, 2 girls, 12.7ms\n",
      "image 353/402 //workspace/dataset/test/images/images-2025-06-10T102720_077_jpg.rf.049489a694a14b8619435241cfdfd71a_orig.jpg: 640x640 4 boys, 2 girls, 12.8ms\n",
      "image 354/402 //workspace/dataset/test/images/images-2025-06-10T102732_916_jpg.rf.a271659b87c0ed46af448936e3b4a5f1_orig.jpg: 640x640 2 boys, 2 girls, 15.1ms\n",
      "image 355/402 //workspace/dataset/test/images/images-2025-06-10T102735_420_jpg.rf.06e4a6e5571d7934b21309b5affa1a35_orig.jpg: 640x640 1 boy, 16.1ms\n",
      "image 356/402 //workspace/dataset/test/images/images-2025-06-10T102814_592_jpg.rf.922db994d15f92b902b2583f4524fbd1_orig.jpg: 640x640 1 boy, 1 girl, 18.1ms\n",
      "image 357/402 //workspace/dataset/test/images/images-2025-06-10T102832_410_jpg.rf.804596bef77660d55ed4ca87ffaa2dbe_orig.jpg: 640x640 1 boy, 17.0ms\n",
      "image 358/402 //workspace/dataset/test/images/images-2025-06-10T102835_212_jpg.rf.e9c8f3ca67381ea802fbffd775ac97f4_orig.jpg: 640x640 1 boy, 3 girls, 15.2ms\n",
      "image 359/402 //workspace/dataset/test/images/images-2025-06-10T102838_360_jpg.rf.0c3402a213cc9ddd6cc12fa9c4f22e52_orig.jpg: 640x640 2 girls, 17.3ms\n",
      "image 360/402 //workspace/dataset/test/images/images-2025-06-10T102910_600_jpg.rf.dae9debff3d3714f1d6843c207ffbe6a_orig.jpg: 640x640 3 boys, 5 girls, 1 woman, 14.7ms\n",
      "image 361/402 //workspace/dataset/test/images/images-2025-06-10T102929_455_jpg.rf.e26e23443a1c35719f1f76241a0393e1_orig.jpg: 640x640 2 boys, 2 girls, 14.6ms\n",
      "image 362/402 //workspace/dataset/test/images/images-2025-06-10T102948_786_jpg.rf.94eb2323084147868949f1eec30c4170_orig.jpg: 640x640 4 boys, 3 girls, 1 woman, 14.6ms\n",
      "image 363/402 //workspace/dataset/test/images/images-2025-06-10T103020_663_jpg.rf.3b64749960d8983f815daf29574c5dad_orig.jpg: 640x640 1 boy, 1 girl, 17.1ms\n",
      "image 364/402 //workspace/dataset/test/images/images-2025-06-10T103039_873_jpg.rf.220637d5da86a1e90bca1f9412c49dbd_orig.jpg: 640x640 1 boy, 2 girls, 1 woman, 16.6ms\n",
      "image 365/402 //workspace/dataset/test/images/images-2025-06-10T103118_500_jpg.rf.4c59cec1502f6a9198f6f1d4dc11210a_orig.jpg: 640x640 1 boy, 2 girls, 19.4ms\n",
      "image 366/402 //workspace/dataset/test/images/images-2025-06-10T103126_079_jpg.rf.7cfd1a6b059d9f91479968ea69c04d62_orig.jpg: 640x640 2 girls, 15.9ms\n",
      "image 367/402 //workspace/dataset/test/images/images-2025-06-10T103145_736_jpg.rf.bfa0b80423aa41f6a26e75c8179d7272_orig.jpg: 640x640 2 boys, 2 girls, 1 woman, 19.1ms\n",
      "image 368/402 //workspace/dataset/test/images/images-2025-06-10T103150_319_jpg.rf.8db5a69d8d437411aa8a2ddf38a7f820_orig.jpg: 640x640 1 boy, 1 girl, 1 woman, 18.8ms\n",
      "image 369/402 //workspace/dataset/test/images/images-2025-06-10T103346_530_jpg.rf.95d9b43c32e0192cd79b077df9982009_orig.jpg: 640x640 1 girl, 1 man, 21.1ms\n",
      "image 370/402 //workspace/dataset/test/images/images-2025-06-10T103421_887_jpg.rf.878da4130849b41b8c965b4f473b9212_orig.jpg: 640x640 4 boys, 2 girls, 2 womans, 19.1ms\n",
      "image 371/402 //workspace/dataset/test/images/images-2025-06-10T103424_085_jpg.rf.2404878393bffc17828082e2ae9f9b68_orig.jpg: 640x640 2 boys, 2 girls, 18.3ms\n",
      "image 372/402 //workspace/dataset/test/images/images-2025-06-10T103432_834_jpg.rf.f7cb5dc8decd72da7480ade44b003546_orig.jpg: 640x640 5 boys, 1 girl, 17.0ms\n",
      "image 373/402 //workspace/dataset/test/images/images-2025-06-10T103541_891_jpg.rf.cb90fd04c99038775e0e3de6979880a4_orig.jpg: 640x640 1 boy, 1 girl, 15.2ms\n",
      "image 374/402 //workspace/dataset/test/images/images-2025-06-10T104750_965_jpg.rf.88d5a9f6d245c948f3763babd3d94780_orig.jpg: 640x640 4 girls, 19.4ms\n",
      "image 375/402 //workspace/dataset/test/images/images-2025-06-10T104800_621_jpg.rf.6272fd594ab315abf644231068d8185b_orig.jpg: 640x640 1 girl, 11.2ms\n",
      "image 376/402 //workspace/dataset/test/images/images-2025-06-10T104803_506_jpg.rf.311f6209bd0dbbf6008ce1fe475cf236_orig.jpg: 640x640 3 boys, 2 girls, 17.5ms\n",
      "image 377/402 //workspace/dataset/test/images/images-2025-06-10T104821_489_jpg.rf.e02dc1ccf263b54c29261e58de7d793b_orig.jpg: 640x640 7 boys, 19.3ms\n",
      "image 378/402 //workspace/dataset/test/images/images-2025-06-10T104911_792_jpg.rf.7bdb375f328c4a8112e8fe07fb274a01_orig.jpg: 640x640 1 boy, 4 girls, 1 woman, 18.0ms\n",
      "image 379/402 //workspace/dataset/test/images/images-2025-06-10T105122_736_jpg.rf.f7c9750913c56d0ae10d0d24694deb0c_orig.jpg: 640x640 4 boys, 2 girls, 18.2ms\n",
      "image 380/402 //workspace/dataset/test/images/images-22-_jpg.rf.9825e2d9b0f1ea81b5c25d587e82ae2a_orig.jpg: 640x640 2 boys, 1 girl, 19.2ms\n",
      "image 381/402 //workspace/dataset/test/images/images-25-_jpg.rf.fcf179098ed7c27f9b1b767a8206fbcb_orig.jpg: 640x640 2 boys, 1 girl, 19.3ms\n",
      "image 382/402 //workspace/dataset/test/images/images-29-_jpg.rf.d19de1f3e122f3ea662995c1e5444b87_orig.jpg: 640x640 2 girls, 16.1ms\n",
      "image 383/402 //workspace/dataset/test/images/images-4-_jpg.rf.0e713e23050bb46f2450b771a457d0cf_orig.jpg: 640x640 2 boys, 2 girls, 19.6ms\n",
      "image 384/402 //workspace/dataset/test/images/images-43-_jpg.rf.b871518511945f06502f36319abd9a11_orig.jpg: 640x640 3 girls, 1 woman, 17.3ms\n",
      "image 385/402 //workspace/dataset/test/images/images-44-_jpg.rf.1afa0d463ad408bce9df3343061964fc_orig.jpg: 640x640 4 boys, 1 girl, 1 man, 18.1ms\n",
      "image 386/402 //workspace/dataset/test/images/images-49-_jpg.rf.62464b0a3617b8a452b8135ea8452a89_orig.jpg: 640x640 5 boys, 11 girls, 2 womans, 15.2ms\n",
      "image 387/402 //workspace/dataset/test/images/images-52-_jpg.rf.86b78a9d52e1bbca2400ac9cd2995bbf_orig.jpg: 640x640 3 boys, 1 girl, 13.9ms\n",
      "image 388/402 //workspace/dataset/test/images/images-56-_jpg.rf.58fdda942057aee5f977ea800783dcd9_orig.jpg: 640x640 1 boy, 1 girl, 16.0ms\n",
      "image 389/402 //workspace/dataset/test/images/images-58-_jpg.rf.b0804d5cfc5217860e1d2767e4fa209e_orig.jpg: 640x640 1 girl, 14.6ms\n",
      "image 390/402 //workspace/dataset/test/images/images-59-_jpg.rf.374944e4feda21e85b63fc0a637b0b80_orig.jpg: 640x640 4 boys, 1 girl, 20.6ms\n",
      "image 391/402 //workspace/dataset/test/images/images-6-_jpg.rf.cbeb1451d36adcf5f6c9edf239909588_orig.jpg: 640x640 3 boys, 5 girls, 21.6ms\n",
      "image 392/402 //workspace/dataset/test/images/images-69-_jpg.rf.65eaaf1b076b66f10eb5fd88ed22fc39_orig.jpg: 640x640 1 girl, 18.5ms\n",
      "image 393/402 //workspace/dataset/test/images/images-73-_jpg.rf.f1d6561f8959a44703ee69985f0d434d_orig.jpg: 640x640 1 boy, 3 girls, 1 man, 1 woman, 18.1ms\n",
      "image 394/402 //workspace/dataset/test/images/images-74-_jpg.rf.715587ac9a165697893b3bda314a5331_orig.jpg: 640x640 3 boys, 4 girls, 16.7ms\n",
      "image 395/402 //workspace/dataset/test/images/images-77-_jpg.rf.094e3d5d7d7b4ce34ad5aa09987d6e8f_orig.jpg: 640x640 1 boy, 5 girls, 1 man, 17.9ms\n",
      "image 396/402 //workspace/dataset/test/images/images-82-_jpg.rf.8d25b5dfa1e2105d58082cdd61bf0060_orig.jpg: 640x640 2 boys, 3 girls, 16.5ms\n",
      "image 397/402 //workspace/dataset/test/images/images-84-_jpg.rf.83d0bad02fe227919f462812552560a6_orig.jpg: 640x640 1 boy, 14.3ms\n",
      "image 398/402 //workspace/dataset/test/images/images-87-_jpg.rf.03310df6335a924d7cb1891fd64f54ef_orig.jpg: 640x640 1 boy, 2 girls, 1 man, 13.6ms\n",
      "image 399/402 //workspace/dataset/test/images/images-89-_jpg.rf.8da39a791dede3956aa8aed0ef36f7d2_orig.jpg: 640x640 2 boys, 1 girl, 2 mans, 13.0ms\n",
      "image 400/402 //workspace/dataset/test/images/images-91-_jpg.rf.3dd47df26d50aedc6d7606f85e8665ac_orig.jpg: 640x640 1 boy, 14.8ms\n",
      "image 401/402 //workspace/dataset/test/images/images-95-_jpg.rf.a9537484e52e0fc52d236b47075c6b4c_orig.jpg: 640x640 2 girls, 12.7ms\n",
      "image 402/402 //workspace/dataset/test/images/images-96-_jpg.rf.ef39a514d2e90fc1aeb5d891de2dd327_orig.jpg: 640x640 1 boy, 14.3ms\n",
      "Speed: 2.3ms preprocess, 15.0ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1m/workspace/runs/detect/predict\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "train_nr = ''\n",
    "\n",
    "best_model_path = f'{home}/runs/detect/train{train_nr}/weights/best.pt'\n",
    "\n",
    "best_model = YOLO(best_model_path)\n",
    "\n",
    "# Validate the model\n",
    "\n",
    "metrics = best_model.val()  # no arguments needed, dataset and settings remembered\n",
    "results = best_model.predict(source = f\"{dataset_path}/test/images\", save = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step # 04 Evaluate fine-tuned YOLOv12 model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls {home}/runs/detect/train{train_nr}/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename=f'{home}/runs/detect/train{train_nr}/confusion_matrix.png', width=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename=f'{home}/runs/detect/train{train_nr}/confusion_matrix_normalized.png', width=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename=f'{home}/runs/detect/train{train_nr}/results.png', width=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision = TP / (TP + FP)\n",
    "\n",
    "Precision is simply true positives out of total detections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Image(filename=f'{home}/runs/detect/train{train_nr}/P_curve.png', width=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall = TP / (TP + FN)\n",
    "\n",
    "\n",
    "\n",
    "Recall is the True Positive out of all Ground Truths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename=f'{home}/runs/detect/train{train_nr}/R_curve.png', width=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename=f'{home}/runs/detect/train{train_nr}/train_batch0.jpg', width=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename=f'{home}/runs/detect/train{train_nr}/val_batch0_pred.jpg', width=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename=f'{home}/runs/detect/train{train_nr}/val_batch1_pred.jpg', width=1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
